\chapter{Alternative Structures}

In the previous chapter we showed how rank and select operation (and later match and enclose) could be used on top of thee different encoding of tree into a bit string.
Here we present data structures, which are fundamentally different in either the primitive operations they use or that they are not-systematic -- they are not based on a single bit string with additional indices.

\begin{enumerate}
	\item Fully-Functional -- a data structure built on top of a sequence of matching parentheses, however it uses different primitives then rank and select;
	\item Tree Covering -- recursive decomposition of the tree into smaller trees which are then encoded;
	\item Universal Succinct Representation -- a structure which provides access to both BP and DFUDS representations at the same time.
\end{enumerate}

There are three direction where the development of succinct data structures for trees continued.
Later in time but close to what we have described is so called fully functional approach which keeps bit string of BP, however it replaces all indices for the primitive operations with a single index.
Rank and select are only implications of more general \emph{linear search operations}; ancestral and child operations are translated to range minimum queries.

Leaving the realm of pure bit strings, we can decompose the tree into smaller trees; we do this recursively.
We represent the trees on different levels in different ways.
There has been many decompositions proposed, trying to maintain the number of smaller trees limited and their sizes balanced.

Finally, there has been an attempt to diminish the differences between several representations -- a succinct structure which provides a view to a BP and DFUDS bit string representation.
Its advantage is it automatically benefits from any new index for either BP or DFUDS representation.

\section{FF}

All the previous data structures were based on operations rank and select, which do not take into account the structure of data which we are representing.
The operations match and enclose work only on proximity of the current vertex.

The first truly powerful operation was $lca$ -- it allows to ask queries which are not restricted to a neighborhood of any vertex.
This was achieved by using a range minimum query; it was also the only place where it proved to be useful.
What if we generalized $rmq$ and replaced our old set of operations thereby?
We did not show the algorithms for $level\_ancestor$ in BP and DFUDS for a reasons: they were too complicated, whereas here in the Fully Functional data structure, it is going to be one of the easiest operations.

\todo{intro}

\subsection{New Operations}

We follow on \ref{ss:rmq-def} and define operations on the array $S$ with function $g$.
\begin{itemize}
	\item $sum(S, g, i, j) = \sum_{k=i}^j g(S[k])$ -- it is equivalent to $G[j] - G[i - 1]$ from \ref{ss:rmq-def}.
	Since $rank$ operation will no longer be a primitive operation, we cannot use it to compute $G[i]$.

	\item Linear search operations:
	\begin{itemize}
		\item $fwd\_search(S, g, i, d) = \min_{j > i} \{j : sum(S, g, i, j) = d\}$ returns the first position such that the difference of values in $G$ is $d$;
		\item $bwd\_search(S, g, i, d) = \max_{j < i} \{j : sum(S, g, j, i) = d\}$ works the same way as $fwd\_search$ but searches in direction towards the beginning of the array.
	\end{itemize}
	
	\item We extend and generalize the range queries:
	\begin{itemize}
		\item $rmq(S, g, i, j) = sum(S, g, 0, i - 1) + \min_{i \le k \le j} sum(S, g, i, k)$ -- returns the value of the minimum in the given range.
		\item $rmq\_size(S, g, i, j) = | \{ i \le k \le j : sum(S, g, i, k) = rmq(S, g, i, j) \} | $ -- returns how many times the minimum occurred in the given range;
		\item $rmq\_rank(S, g, i, j, k)$ -- returns how many minima there are in the given range up to position $k$.
		\item This can be directly implemented using $rmq$ and $rmq\_size$:
		\begin{algorithmic}
			\Function{rmq\_rank}{$S, g, i, j, k$}
				\If{$rmq(S, g, i, j) \ne rmq(S, g, i, k)$}
					\State \Return{$0$}
				\Else
					\State \Return{$rmq\_size(S, g, i, k)$}
				\EndIf
			\EndFunction
		\end{algorithmic}
		\item $rmq\_select(S, g, i, j, n) = \min_{k \ge i} rmq\_rank(S, g, i, j, k) = n$ -- returns position of $n$-th minimum in range $i, j$;
		\item $RMQ, RMQ\_size, RMQ\_rank, RMQ\_select$ are defined similarly.
	\end{itemize}
\end{itemize}

We show how it is possible to realize all operations which we used as primitives in the previous succinct data structures only by searches and range queries on calculated arrays.
We immediately get a data structure for BP and DFUDS representations which supports all queries which were supported before.
Later we show that even more operations which were impossible or required complicated indices are now feasible.

Ranking and selecting:
\begin{align*}
	rank_1(i) &= sum(S, \phi, 0, i) \\
	select_1(n) &= fwd\_search(S, \phi, 0, n) \\
	rank_0(i) &= sum(S, \psi, 0, i) \\
	select_0(n) &= fwd\_search(S, \psi, 0, n)
\end{align*}

Operations on sequences of matching parentheses:
\begin{align*}
	find\_close(i) &= fwd\_search(S, \pi, i, 0) \\
	find\_open(i) &= bwd\_search(S, \pi, i, 0) \\
	enclose(i) &= bwd\_search(S, \pi, i, 2) \\
	enclose(i_1, i_2)&\ \textrm{stays the same} \\
	rmqi(E, i, j) &= rmq\_select(S, \pi, i, j, 1) \\
	RMQi(E, i, j) &= RMQ\_select(S, \pi, i, j, 1)
\end{align*}

\subsection{The Data Structure}

As we are describing a universal structure independent on its specific usage for representing trees, we again use $N$ to denote the size of the bit string.
We assume a general $\pm 1$-function $g$.

The bit string $S$ is first split into blocks, which are roughly of polylogarithmic size.
Each block is then split into small blocks of size $\frac{\log N}{2}$.
Queries withing a small block are processed by look-up tables.
If the query spans multiple small blocks withing a single block, the queries are handled by min-max trees.
The queries spanning multiple blocks are answered by a macro structure.

\subsubsection{Small Blocks}

The lowest level of small blocks of size $b = \frac{\log N}{2}$ is the easiest.
The small blocks are small enough to by used as indices of precomputed look-up tables, which answer all queries in constant time.
We require the following look-up tables:
\begin{itemize}
	\item For given range $i, j$ returns the sum $v$.
	\item For given position $i$ and value $d$, returns the first next and previous position $p$ where the difference is $d$.
	\item For given range $i, j$ returns the local value $v$ of minimum and maximum.
	\item For given range $i, j$ returns the number $n$ of occurrences of minima and maxima.
	\item For given range $i, j$ and $n$, return the position $p$ of $n$-th minimum and maximum in the small block.
\end{itemize}
The values of $i, j, n$ are in range $[0, b - 1]$; the values $d, v$ are in range $[-b, b]$; the value of $p$ is in range $[0, b - 1]$ plus one special value indicating that such position does not exist in the small block.
Together we have 8 precomputed tables; each of them requires $O(\sqrt{N} \log N \log \log^3 N)$ bits of memory.

The tables are used to answer all range operations whenever $i$ and $j$ are in the same small block.
In case of searches, they also determine that the answer lay outside of the block.

\subsubsection{Min-Max Tree}

Each block contains $k^c$ small blocks for $k = \frac{\log N}{\log \log N}$ and an arbitrary constant $c \le 1$.
The $l$-th block covers an interval of $B = b k^c$ bits spanning from position $(l-1) B$ to $l B - 1$.
To simplify the description we isolate the block by shifting the offsets so that it covers the range $[0, B - 1]$.
We also assume that the values in the array $G$ are in range $[-B, B]$; the global values can be computed simply by adding the value $G[(l-1) B - 1]$.
This does not have any impact on correctness of the operations on the block level.

We build a perfect $k$-ary tree on top of the sequence of small blocks; we call it a \emph{min-max tree}.
The tree has a constant height equal to $c$.
Its leaves represent the information from small blocks provided by look-up tables; the inner nodes aggregate information from their children.
We store the following values in each node spanning the range $i, j$:
\begin{itemize}
	\item $e = sum(S, g, 0, j)$ -- the value at the end of the range;
	\item $m = rmq(S, g, i, j), ms = rmq\_size(S, g, i, j)$ -- the value of minimum and how many times it occurred;
	\item $M = RMQ(S, g, i, j), Ms = RMQ\_size(S, g, i, j)$ -- the value of maximum and how many times it occurred.
\end{itemize}

The aggregation in inner node is obvious: the last value for $e$; minimum for $m$, maximum for $M$; sum for $ms$ and $Ms$.
All values within a node require only $O(\log b)$ bits each.

We number the nodes of the tree in a heap-like fashion and store their values in arrays $e[\cdot], m[\cdot], ms[\cdot], M[\cdot], Ms[\cdot]$.
Note that all children of a node are stored together in consecutive $O(k \log \log N) = O(\log N) = c' b$ bits of memory, for a constant $c'$.
There are $\frac{k^c}{k-1} = O(k^{c-1})$ nodes, each of them requires $O(\log b)$ bits.
The density of the tree built on top of the block is $\frac{O(k^c \log b)}{k^c b} = O(\frac{\log b}{b}) = o(1)$.

\bigskip

We show how we support the operations on block level using traversal of the min-max tree and possibly a delegation to small blocks.
By $b(i) = \lfloor\frac{i}{b}\rfloor$ we denote the index of the small block which contains $i$.
To simplify the notation we assume that the node numbers of leaves are the same as indices of the small blocks with which they are associated.

\begin{algorithmic}
\Function{sum\_block}{$S, g, i, j$}
	\State $x \gets e[b(i - 1) - 1] + sum\_small\_block(S_{b(i - 1)}, g, 0, (i - 1) \% b)$
	\State $y \gets e[b(j) - 1] + sum\_small\_block(S_{b(j)}, g, 0, j \% b)$
	\State \Return{$y - x$}
\EndFunction
\end{algorithmic}

\paragraph{Search Operations}

\begin{lemma}\label{l:search}
	The result of search operations starting at $i$ and looking for a difference $d$ (which does not have a result in the small block $h$ containing $i$) lays in the first small block $j$ such that $m[j] \le v = sum(S, g, 0, ) + d \le M[j]$.
	We say that the value $v$ is covered by the extremes (minimum and maximum).
	
	No small block in subtree of a node of the min-max tree whose extremes do not cover the value $v$ contains the answer.
\end{lemma}
\begin{proof}
	The function $g$ which is used for calculating the array $G$ requires that $| G[k] - G[k-1] | \le 1$, which is the property introduced in \ref{ss:rmq-def}.
	From that follows that each small block contains all values between the minimum and maximum in the array $G$.
	The first small block which satisfies the condition on the extremes contains the desired value $v$.
	
	Moreover, if $v < \textrm{minimum of the rest of the block } b(i)$, then it is sufficient to find the first small block $j$ such that $m[j] \ge v$ because $M[j] \ge m[j-1]$.
	Similar statement holds for $v$ greater than the maximum.
	
	A leaf is associated with a small block and so its extremes are the same as of the small block.
	By contradiction, if a small block in a subtree contained the answer, the extremes of the leaf would cover $v$.
	An internal node aggregates the minimum and maximum of its subtree and so if its child contained the answer, the aggregation would assure that the extremes of the node cover $v$.
\end{proof}

When we process the search operations, we first check the containing small block for an answer.
If the answer is present there, we return the position and end.

Otherwise, we compute the desired value $v$ and traverse the tree up until we find the answer or until we reach the root.
If we get to root, we signal that this block does not contain the answer by returning a special value.
When we are in node $a$, we check the extremes of its siblings in the direction in which we are searching.
This check can be performed in time $c'$ using precomputed look-up tables as all siblings are stored in consecutive $c' b$ bits.

When we find such sibling, we move to it and start descending to the first child whose extremes cover the value $v$.
Finding such child again requires time $c'$ and another set of precomputed tables.
Once we descend into a leaf $j$, we compute the value $d' = v - e[j - 1]$ which we are going to look for in the associated small block using a look-up table. 
As the height of the tree is $c$, the whole operation takes at most $O(2 c c') = O(1)$ steps.

By an asterisk we denote a repeated application of look-up table on a list of consecutive nodes in the tree.

\begin{algorithmic}
\Function{fwd\_search\_block}{$S, g, i, d$}
	\State $p \gets fwd\_search\_small\_block(S_{b(i)}, g, i \% b, d)$
	\If{$p \ne -1$}
		\State \Return{$p + b(i) b$}
	\Else
		\State $v \gets e[b(i) - 1] + sum\_small\_block(S_{b(i)}, g, 0, i \% b) + d$
		\State $n \gets b(i)$ \Comment{The current node, initialized to be a leaf}
		\While{$(j \gets table_1^*[right\_siblings(n), v]) = -1$} \Comment{Sibling covering $v$}
			\State $n \gets parent(n)$
			\If{$is\_root(n)$}
				\State \Return{$-1$}
			\EndIf
		\EndWhile
		\While{$\boolnot is\_leaf(j)$}
			\State{$j \gets table_1^*[children(j), v]$} \Comment{The first child covering $v$}
		\EndWhile
		\State $d' \gets v - e[j - 1]$ \Comment{The remaining difference}
		\State \Return{$j b + fwd\_search\_small\_block'(S_{j}, g, 0, d')$} \Comment{$d'$ in block $j$}
	\EndIf
\EndFunction
\end{algorithmic}

Note that the final $fwd\_search\_small\_block'$ is the standard $fwd\_search\_small\_block$ altered to allow the answer $0$, which is prohibited by the original definition.
This can be handled with a simple check for $sum\_small\_block(S_{j}, g, 0, 0) = d'$.

\paragraph{Range Operations}

The range operations works similarly, they traverse the tree up gathering information on the way, and then descend down in case of range select operations.
If the query is fully contained within a small block, we get the answer from a lookup table and end.

First we find the lowest common ancestor $a$ of the leaves $u = b(i)$ and $v = b(j)$.
We keep two lists of partial answers, one for each branch starting at $u$ and $v$.

We first ask the small blocks for their answers and remember them in their respective lists.
In a loop until $u$ becomes $a$ we get the answers from the right siblings of $u$ and the left siblings of $v$, we remember them in the lists and move to their parents.
In the last iteration the left siblings and right siblings overlap; therefore we store the answers in only one of the lists.
Note that each list contains at most $c c'$ values as the table look-ups aggregate the answers.

We combine the lists into a single list and find the answer to the queries in it.
For $rmq$ we return the minimum of the list.
For $rmq\_size$ we sum $ms$ for those nodes whose minimum is equal to $rmq$.
In case of $rmq\_select$ we find the first node in the list such that the prefix sum of $ms$ is greater or equal to $n$.
We descend in such node to a child whose left siblings including itself sum up to $n$, and repeat until we get to a leaf.
Once we are in a leaf, we use a look-up table to solve the select there.

By the same argument as in search operations, the running time is $O(2 c c') = O(1)$.

\begin{algorithmic}
\Function{rmq\_list}{$S, g, i, j$} \Comment{$b(i) \ne b(j)$}
	\State $left \gets []; right \gets []$ \Comment{Initialization of the empty lists}
	\State $left.append(\{node{:}\ b(i), m{:}\ rmq\_small\_block(S_{b(i)}, g, i \% b, b - 1) + e[b(i) - 1],\allowbreak ms{:}\ rmq\_size\_small\_block(S_{b(i)}, g, i \% b, b - 1), M {:}\ \ldots, Ms{:}\ \ldots \})$
	\State $right.prepend(\{node{:}\ b(j), m{:}\ \ldots, ms{:}\ \ldots, M{:}\ \ldots, Ms{:}\ \ldots\})$
	\State $a \gets lca(u, v)$
	\While{$parent(u) \ne a$} \Comment{$u, v$ in different subtrees}
		\State $left.append(rmq\_info(S, g, u, "right"))$
		\State $right.prepend(rmq\_info(S, g, v, "left"))$
	\EndWhile
	\State $left.append(rmq\_info(S, g, u, v))$ \Comment{Between $u$ and $v$}
	\State $left.append(right)$
	\State \Return{$left$}
\EndFunction
\end{algorithmic}

The function $rmq\_info$ returns list of touples containing: node number, minimum, maximum, number of minima, number of maxima.
It processes the siblings of given vertex from left to right producing aggregated info per each $\frac{k}{c'}$ processed siblings.

\begin{algorithmic}
\Function{rmq\_block}{$S, g, i, j$}
	\If{$b(i) = b(j)$}
		\State \Return{$rmq\_small\_block(S_{b(i)}, g, i \% b, j \% b) + e[b(i) - 1]$}
	\Else
		\State $list \gets rmq\_list(S, g, i, j)$
		\State $m \gets \infty$
		\ForAll{$I \gets list$}
			\State $m \gets \min(m, I.m)$
		\EndFor
		\State \Return{$m$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{rmq\_size\_block}{$S, g, i, j$}
	\If{$b(i) = b(j)$}
		\State \Return{$rmq\_size\_small\_block(S_{b(i)}, g, i \% b, j \% b)$}
	\Else
		\State $list \gets rmq\_list(G, i, j)$
		\State $m \gets rmq\_block(G, i, j)$
		\State $ms \gets 0$
		\ForAll{$I \gets list$}
			\If{$I.m = m$}
				\State $ms \gets ms + I.ms$
			\EndIf
		\EndFor
		\State \Return{$ms$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{rmq\_select\_block}{$S, g, i, j, n$} \Comment{Assuming $n$-th min exists}
	\If{$b(i) = b(j)$}
		\State $p \gets rmq\_select\_small\_block(S_{b(i)}, g, i \% b, j \% b, n)$
		\State \Return{$b(i) b + p$}
	\Else
		\State $list \gets rmq\_list(G, i, j)$
		\State $m \gets rmq\_block(G, i, j)$
		\State $ms \gets 0$
		\ForAll{$I \gets list$}
			\If{$I.m = m$}
				\State $ms \gets ms + I.ms$
				\If{$ms \ge n$}
					\State \Break
				\EndIf
			\EndIf
		\EndFor
\algstore{rmqselect}
\end{algorithmic}

Now $I$ contains aggregated information for up to $\frac{k}{c'}$ nodes.
Using a table look-up we find the correct node $p$ and also a new $n$ which applies to the node $p$.

\begin{algorithmic}
\algrestore{rmqselect}
		\State $(p, n) \gets table_1[left\_siblings\_inclusive(I.node), ms - n]$
		\While{$\boolnot is\_leaf(p)$}
			\State $(p, n) \gets table_1^*[children(p), n]$
		\EndWhile
		\State $x \gets max(i, p b) \% b; y \gets min(j, (p+1) b -1) \% b$
		\State \Return{$b(p)b + rmq\_select\_small\_block(S_{p}, g, x, y)$}
	\EndIf
\EndFunction
\end{algorithmic}

There are 8 look-up tables necessary for traversal and processing the tree.
The tables have an additional (non-mentioned) parameter which restricts the size of the block to be processed.
\begin{itemize}
	\item $fwd\_search$ -- looking for the first occurrence of $v$ in block of consecutive nodes.
	$bwd\_search$ requires another table returning the position of the last occurrence $v$.
	\item $rmq\_info$ -- aggregates info from consecutive nodes; four tables are necessary in total.
	\item $rmq\_select$ -- looking for $n$-th minimum returning index of the node among its siblings and number of minima in its preceding siblings.
	$RMQ\_select$ requires a similar table for maxima.
\end{itemize}
All of them work on blocks of $b$ bits with $O(1)$ parameters of size $O(\log k^c) = O(\log \log N)$.

\subsubsection{Macro Structure}

The macro structure starts with five arrays which summarize the results from the underlying blocks (root nodes of their min-max trees).
\begin{itemize}
	\item $e[i]$ --  the value at the end of the block $i$;
	In comparison to the block level, here we store the absolute values (the same for minima and maxima).
	Each element has size of $O(\log N)$ bits.
	\item $m[i]$, $M[i]$ -- the minimum and maximum values of the block $i$.
	\item $ms[i]$, $Ms[i]$ -- the number of occurrences in the block $i$.
	Each element is of size $O(\log\log N)$ bits.
\end{itemize}

First we deal with $sum$.
We use $B(i) = \lfloor\frac{i}{B}\rfloor$ to refer to the index of a block containing $i$.
The algorithm is very similar to $sum\_small\_block$.

\begin{algorithmic}
\Function{sum\_block}{$S, g, i, j$}
	\State $x \gets e[B(i - 1) - 1] + sum\_block(S_{B(i - 1)}, g, 0, (i - 1) \% B)$
	\State $y \gets e[B(j) - 1] + sum\_block(S_{B(j)}, g, 0, j \% B)$
	\State \Return{$y - x$}
\EndFunction
\end{algorithmic}

\paragraph{A structure for the Search Operation}

For each block $i$ we define the arrays of \emph{left-to-right minima} $lrm(i)$ such that $lrm(i)[0] = i, lrm(i)[j] < lrm(i)[j+1], m[lrm(i)[j]] < m[lrm(i)[j+1]]$.
Similarly we define left-to-right maxima arrays $LRM(i)$.

We can use the $lrm$ and $LRM$ arrays to implement $fwd\_search(S, g, i, d)$.
Let's assume that the result is not in block $B(i)$, if it was, we can solve it on the block level.
We look for a value $v = sum(S, g, 0, i) + d$ in the block $B(i) + 1$ using the min-max tree on the block level to find the answer.
If the answer exists in the block $B(i)+1$, we return the position and end.
Otherwise, either $v > M[B(i) + 1]$ or $v < m[B(i) + 1]$; in the first case we follow with the search in $lrm$, else in $LRM$.
We either find a block $j$ which contains the answer, and we finish the operation on the block level, or report a failure.

\begin{algorithmic}
\Function{fwd\_search}{$S, g, i, d$}
	\State $p \gets fwd\_search\_block(S_B(i), g, i \% B, d)$
	\If{$p \ne -1$}
		\State \Return{$B(i) B + p$}
	\Else
		\State $v \gets sum(S, g, 0, i) + d$
		\State $n \gets B(i) + 1$
		\If{$v < m[n]$}
			\State $j \gets lrm\_search(n, v)$
		\ElsIf{$v > M[j]$}
			\State $j \gets LRM\_search(n, v)$
		\Else
			\State $j \gets n$
		\EndIf
		\If{$j = root$}
			\State \Return{$-1$}
		\Else
			\State $d' \gets v - sum(S, g, 0, B(j) B - 1)$
			\State \Return{$B(j) B  + fwd\_search\_block'(S_{B(j)}, g, 0, d')$}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

Note that the final $fwd\_search\_block'$ is the standard $fwd\_search\_block$ altered to allow the answer $0$.

\bigskip

It remains to solve the $lrm\_search(n, v)$ in constant time ($LRM_search$ is similar).
Every node has at most $\frac{N}{B}$ ancestors; also for every node $i$ (except for the root) $m[i] - m[j] \le N$.
We could try to encode each array $lrm(i)$ as a bit string $lrm_i$ of size $N$ such that it has ones at positions $m[i] - lrm(i)[j] \forall j \ge 0$.
Then we could use the $pred_1(lrm_n, m[n] - v)$ operation to find a node $j$ which contains the value $v$.
However such representation leads to $O\left(\left(\frac{N}{B}\right)^2\right)$ elements in all $lrm$ arrays combined.

We observe that $lrm$ arrays are alike.
\begin{lemma}
	Let block $a$ be in two different sequences $lrm(i): lrm(i)[x_i] = a$ and $lrm(j): lrm(j)[x_j] = a$, then all following values in the sequence are the same: $lrm(i)[x_i+k] = lrm(j)[x_j+k] \forall k \ge 0$.
\end{lemma}

\label{Tlrm}
We use $lrm$ to construct a tree $T_{lrm}$ which is a trie of reversed $lrm$ sequences with an artificial root with assigned minimum $m[root] = -\infty$.
The properties of the tree are:
\begin{enumerate}
	\item $m[i] > m[parent(i)]$;
	\item $i < parent(i)$;
	\item the tree has $\frac{N}{B} + 1$ nodes.
\end{enumerate}

The search for a value $v$ in $lrm(n)$ is transformed to a search for an ancestor $n'$ of $n$ in the tree $T_{lrm}$ such that $m[n'] \le v$.
We split the search into two parts:
\begin{enumerate}
	\item a search in $lrm$ restricted to powers of two;
	\item a search in $lrm$ with a bounded distance.
\end{enumerate}

We reduce the $lrm$ arrays to contain at most $\log N$ elements each.
$$lrm'(i)[j] = lrm(i)[2^j] \forall j \ge 0$$
Then we represent the arrays as bit strings as we proposed earlier.
We also store an array of at most $\log N$ elements translating $j$-th one to a node number.

After the first jump we have an estimate of depths and heights and also a bound on number of ancestors which we have to search through.
\begin{align*}
depth(n'') - depth(n') &< depth(n) - depth(n'') \\
height(n'') &\ge depth(n) - depth(n'') \\
depth(n'') - depth(n') &\le height(n'')
\end{align*}

We decompose the tree iteratively to paths; in each step the longest path $p$ is removed.
\begin{align*}
height(start(p)) = length(p)
\end{align*}
We extend the path $p$ into a ladder $l$ by including $length(p)$ of ancestors of $start(p)$.
With the previous bound, we are guarantied that $n'$ is in the same ladder as $n''$.
There are as many ladders as leaves of the tree $T_{lrm}$.
All ladders combined contain $O\left(\frac{N}{B}\right)$ nodes.

For each node of the tree, we have a mapping of its number to the representation of the ladder it belongs to and its position withing the ladder.
The ladder is a bit string $ladder_l$ of size $N$ containing $length(l)$ ones at positions $m[end(l)] - m[l[j]] \forall j \ge 0$ (the leaf is at position $0$).
In a ladder we perform a successor search for a value $m[end(l)] - v$ and translate the position back to node number.

\begin{algorithmic}
\Function{lrm\_search}{$n, v$}
	\State $n'' \gets ancestor_1[n][rank_1(pred_1(lrm_n, m[n] - v))]$
	\State $(l, p) \gets ladder[n'']$ \Comment{The index of the ladder and the position within}
	\State $n' \gets ancestor_2[l][rank_1(succ_1(ladder_l, m[end[l]] - v))]$
	\Return{$n'$}
\EndFunction
\end{algorithmic}

There are several bit strings and tables:
\begin{itemize}
	\item $lrm_n$ -- a collection of $\frac{N}{B}$ bit strings of size $N$ containing $\le \log N$ ones.
	They can be represented using an indexed dictionary in space $O(\frac{N}{B}) (\log {N \choose \log N} + o(\log N) + O(\log \log N)) = O(\frac{N}{B} \log^2 N)$, which requires $c \ge 3$.
	\item $ancestor_1$ -- a collection of $\frac{N}{B}$ tables mapping $i$-th ancestor in $lrm'$ to a node number.
	Each of $\log N$ elements has size $\log N$; the total space is therefore $O(\frac{N}{B} \log^2 N) = o(N)$ as long as $c \ge 3$.
	\item $ladder$ -- a table mapping node number to ladder index and position within.
	It contains $\frac{N}{B}$ elements of size $\log{N}$.
	\item $ladder_l$ -- a collection of up to $\frac{N}{B}$ bit strings of size $N$ containing in total $2 \frac{N}{B}$ ones.
	Using Jensen's inequality, we get an upper bound of $O(\frac{N}{B} \log N)$; the costs of the dictionaries $o(\frac{N}{B}) + O(\frac{N}{B} \log \log N)$ are negligible.
	\item $end$ -- maps ladder index to a node number of its leaf.
	It contains $\le \frac{N}{B}$ elements of size $\log{N}$.
	\item $ancestor_2$ -- a collection of $\le \frac{N}{B}$ tables which for a given ladder index translates the position in the ladder to a node number.
	It contains $\le \frac{N}{B}$ elements of size $\log{N}$.
\end{itemize}
The total space of this structure is $O(\frac{N}{B} \log^2 N)$.

\paragraph{Range Queries}

We can use the structure from lemma \ref{lemma:rmq2} with a minor changes (it works with values rather than positions) to solve $rmq$ and $RMQ$.
This time it is sufficient to use it only once provided that we set $c \ge 3$.

The structure for $rmq$ is however not suitable for querying the total number of occurrences of minimum nor selecting $n$-th of such.
If we augmented each precomputed interval of size $2^k$ with the number of occurrences, we would still fit in the same space, however computing the number of occurrences cannot easily combine information from only two intervals like the $\min$ function does.
In this case, we have to use the inclusion-exclusion principle: adding the numbers of occurrences in both intervals and subtracting the number of occurrences in the overlapping part.
The size of the overlapping part is not a power of two in general and so, its number of occurrences is not precomputed, which leads to a recursion and running time $O(\log N)$.

\paragraph{Bitmaps of Minima and Maxima}

For each minimum $r$ (and maximum $R$, but we formulate the structure for minima only as the maxima case is symmetric) we define two bit strings:
\begin{itemize}
	\item $p_r[i] = (m[i] = r)$; $p_r$ is a bitmap which marks blocks containing the minimum $r$.
	For purposes of $rmq\_size$ and $rmq\_select$ only these are interesting because the other blocks either do not contain the minimum $r$ (if their minimum is $> r$) or cannot be accounted (if their minimum $r' < r$, then a query spanning over this block has a minimum $r'$ rather than $r$).
	Each bit string has size $N$ and contains $k_r$ ones (which is number of blocks with such minimum).
	Because $p_r$ is a partitioning of $[0, N - 1]$, $\sum{r} k_r = \frac{N}{B}$.
	\item $s_r$ from the lemma \ref{l:sps} applied on numbers of occurrences of the minimum $r$ in blocks.
	The bit string has size up to $N$ (we extend it to $N$) and contains $k_r$ ones.
\end{itemize}
There exist at most $\frac{N}{B}$ distinct minima and therefore the same number of bit strings.

Using these two sets of arrays, we can support $rmq\_size$ and $rmq\_select$.
It only necessary for us to focus on the span of the query as we the prefix and suffix can be handled using the min-max trees on the block level.
Without loss of generality we assume that the query has only span.

\begin{algorithmic}
\Function{rmq\_size}{$S, g, i, j$}
	\State $r \gets rmq(S, g, i, j)$
	\State $y \gets rank_1(p_r, B(j)); x \gets rank_1(p_r, B(i) - 1)$
	\State \Return{$prefix\_sum(s_r, y) - prefix\_sum(s_r, x)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{rmq\_select}{$S, g, i, j, n$}
	\State $r \gets rmq(S, g, i, j)$
	\State $s \gets prefix\_sum(s_r, rank_1(p_r, B(i) - 1))$ \Comment{Prefix sum before $i$}
	\State $a \gets select_1(p_r, prefix\_leq(s_r, s + n))$ \Comment{Block containing the $n$-th minimum}
	\State $n' \gets s + n - prefix\_sum(s_r, a - 1)$ \Comment{$n$-th within block $a$}
	\State \Return{$B a + rmq\_select\_block(S_a, 0, B - 1, n')$}
\EndFunction
\end{algorithmic}

In the general case, the minimum $r$ might have occurred only in prefix or suffix and not span; then the bit string $m_r$ does not exist and we simply skip the span.
In case of select, we first check using $rmq\_size$ in which part the answer lays and then call $rmq\_size$ on that part with altered $n$ by the number occurrences in the preceding parts.

We represent the collections of bit strings $p_r$ and $s_r$ using the previous lemma. \todo{structure}
The space complexity of $p_r$ is $O\left(\frac{N}{B} \log \frac{N}{B}\right) = o(N)$; in case of $s_r$ the space complexity is $O\left(\frac{N}{B} \log N \right) = o(N)$.

\subsection{$\pm 1$ Functions Revisited}

Although there are 9 functions $g$ mapping ${0, 1} \to {-1, 0, 1}$, we only use 3 ($\pi, \phi, \psi$).
Answers to all operations except for searches with parameters $\phi$ and $\psi$ can be answered using the structure for $\pi$.

For the operation $sum$, the following identities hold:
\begin{gather*}
	sum(S, \pi, i, j) = sum(S, \phi, i, j) - sum(S, \psi, i, j) \\
	sum(S, \phi, i, j) + sum(S, \psi, i, j) = j - i + 1
\end{gather*}

We can find the explicit formula for either of them:
\begin{align*}
	sum(S, \phi, i, j) &= \frac{(j - i + 1) + sum(S, \pi, i, j)}{2} \\
	sum(S, \psi, i, j) &= \frac{(j - i + 1) - sum(S, \pi, i, j)}{2}
\end{align*}

Because the sums of $g \in \{\phi, \psi \}$ are monotonous, we can reduce all range queries to sums, searches and arithmetics.
In the range $i, j$, the minimum occurs for the first time at $i$ and maximum occurs the last time at $j$.
All occurrences are continuous.
\begin{align*}
	rmq(S, g, i, j) &= sum(S, g, 0, i) \\
	RMQ(S, g, i, j) &= sum(S, g, 0, j) \\
	rmq\_size(S, g, i, j) &= \min(j + 1, fwd\_search(S, g, i, 1)) - i \\
	RMQ\_size(S, g, i, j) &= j - \max(i - 1, bwd\_search(S, g, j, 1)) \\
	rmq\_select(S, g, i, j, k) &= i + k - 1 \\ 
	RMQ\_select(S, g, i, j, k) &= j - k + 1
\end{align*}

\subsubsection{Search Operations}

The only non-trivial operations are $fwd\_search$ and $bwd\_search$.
They cannot be reduced and must be supported extra.

\todo{phi and psi search operations}

\subsection{Extension of BP}

We focus on encoding the tree as balanced parenthesis, however instead of the traditional indices for rank and select, we use the new data structure.
We have already shown how to reduce all old primitive operations to the new one, which implies that everything shown in the section \todo{ref} is support here too.
Because sum, search, and range operations are more general, we can support more operations.

Child operations -- $degree$, $child_rank$ and $child_select$, which are important for basic navigation in the tree, were only available with a specialized index.
They were also the reason for using DFUDS representation.
Using the generalized $rmq$, we present their simple implementation.
The key observation is that in a representation of a subtree of vertex $i$ with omitted terminal parenthesis, the occurrences of minimum correspond to the terminal parenthesis of the children of $i$.

\begin{algorithmic}
\Function{degree}{$i$}
	\State \Return{$rmq\_size(S, \pi, i, find\_close(i) - 1)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{child\_rank}{$i$}
	\State $p \gets parent(i)$
	\State \Return{$rmq\_rank(S, \pi, p, find\_close(p), find_close(i))$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{child\_select}{$i, n$}
	\State \Return{$find\_open(rmq\_select(S, \pi, i, find\_close(i), n))$}
\EndFunction
\end{algorithmic}

Level ancestor operation is a straightforward generalization of enclose.

\begin{algorithmic}
\Function{level\_ancestor}{$i, d$}
	\State \Return{$bwd\_search(S, \pi, i, d + 1)$}
\EndFunction
\end{algorithmic}

We can also support local navigation on vertices on the same level $l$ within a restricted subtree rooted in a vertex $r$.

\begin{algorithmic}
\Function{level\_first}{$r, l$}
	\State \Return{$fwd\_search(S, \pi, r, l - depth(r))$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{level\_last}{$r, l$}
	\State \Return{$find\_open(bwd\_search(S, \pi, find_close(i), l - depth(r))$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{level\_next}{$r, i$}
	\State \Return{$fwd\_search(S, \pi, find\_close(i), 0)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{level\_prev}{$r, i$}
	\State \Return{$find\_open(bwd\_search(S, \pi, i, 0))$}
\EndFunction
\end{algorithmic}

\bigskip

The only operations which are not supported by this structure are: $level\_size$, $level\_rank$, $level\_select$, $lo\_rank$, and $lo\_select$.
All of them would require a better handling of the level structure, something like $fwd\_search$ with parametrized occurrence.
If these operations are crucial, LOUDS representation supports them.

\section{TC}

All structures which we have seen have several things in common; they are combined from two or three levels of different representations:
\begin{itemize}
	\item On the lowest level they have small blocks of size less than $\log n$ bits.
	With such size, it is possible to precompute results to all possible queries which are contained in the small block.
	\item Then an intermediate level follows which connects several blocks together to blocks of size $\log^c n$ bits.
	It often uses the property that the pointers withing such block require only $O(\log \log n)$ bits.
	\item Finally, the macro level which spans the whole structure connects individual blocks.
	It usually uses a classical non-succinct structure provided that it requires only $O(n \log^{c'} n)$ bits where $c' < c$.
\end{itemize}

So far the small blocks and blocks were chunks of a bit string representation of the tree.
They were designed with the regular size and offset in one's mind instead of being connected to the structure of the tree.
The representation of a single vertex, let alone the sequence of its children, was covered by multiple different (small) blocks.

The tree covering approach respects the structure of the tree by its decomposition into components of a bounded size.
Not dissimilar to the other representations, it also uses three levels and with bit strings on the lowest one and a pointer structure on the top one.

\subsection{Decomposition Algorithm}

Several decomposition algorithms have been proposed; they are parametrized by a target number of vertices $B$ to be present in a component.
The first algorithm \todo{ref} decomposed the tree into components of size $[B, 3 B - 2]$ with the exception of the component containing the root, which could have been undersized.
Such decomposition cannot exist unless we allow the components to overlap; more specifically to overlap in their common root.
From the bounds, it is clear that there exist $O(\log n)$ components.

This decomposition was used and lead to a succinct data structure which at the time of its introduction supported more operations than BP (child operations) or DFUDS (depth).
The problem of the decomposition was that the components were connected together in too many ways and the structure had to handle many cases.

Later a different decomposition was proposed which removes the lower bound on the sizes of the components while retaining their asymptotic number in exchange for restriction on how components can be connected with each other.
There can be at most one edge per component which connects it to a vertex in a different component.
Note that a stronger claim of no such edge existing does not provide a decomposition in a general case (e.g. a tree with depth greater than $2 B$).

We call a vertex \emph{heavy} if its subtree contains at least $B$ vertices.
All ancestors of a heavy vertex are heavy and therefore heavy vertices form a subtree $T_{heavy}$ of the tree $T$.
We call a heavy vertex \emph{branching vertex} if it has at least two heavy children, and \emph{branching edges} are the edges from a branching vertex to its heavy children.

\begin{lemma}
	There are $O\left(\frac{n}{B}\right)$ branching vertices and branching edges in a tree $T$.
\end{lemma}
\begin{proof}
	The tree $T_{heavy}$ has at most $\frac{n}{B}$ leaves (we call them \emph{heavy leaves}) as each leaf is a root of a subtree of $T$ which contains at least $B$ vertices.
	Each branching node connects at least two heavy subtrees containing each at least one heavy leaf; therefore their number must be less than the number of all heavy leaves.
	The number of branching edges is the same as the number of heavy leaves plus number of branching vertices minus one which can be seen after contracting non-branching edges in the tree $T$.
\end{proof}

The algorithm works in DFS post-order; it first recursively processes all children of a vertex $v$ before solving $v$ itself.
A leaf starts its own temporary component.
From the recursion a set of \emph{permanent} and up to one \emph{temporary} component is returned.
The way how they are merged together depends on number of heavy children.

We distinguish three cases:
\begin{enumerate}
	\item If a vertex $v$ does not have any heavy children, then all children are part of a temporary component.
	Children are processed from left to right; their temporary components are merged with $v$ and potentially with components of their right siblings.
	When the size of a component is at least $B$, we declare it permanent.
	If at least one component was declared permanent, we declare all of them as permanent even though the last one can be undersized.
	\item If a vertex $v$ has exactly one heavy child $u$, then we process it in a similar way as the case (1).
	If $u$ is part of a temporary component, nothing changes; otherwise we simply skip it.
	\item If a vertex $v$ has two or more heavy children (it is a branching vertex), the temporary components containing the heavy children are declared permanent no matter what size they have.
	All non-heavy children are split into intervals by the heavy children and processed as in case (1) while declaring all their components as permanent.
	If the vertex $v$ does not have any non-heavy children, then it forms a permanent component of size one.
\end{enumerate}

Several invariants hold during the course of the algorithm:
\begin{itemize}
	\item Size of a permanent component is less than $2 B - 2$; size of a temporary component is less than $B$.
	A temporary component is only merged with other temporary components; it is declared permanent when its size is at least $B$.
	Because we merge a temporary component with the parent first and then with its right siblings one by one, its size will never be greater than $(B - 2) + 1 + (B - 1)$, at which point it is declared permanent.
	\item If a vertex is shared among multiple components, it is their root.
	When such situation happens in the algorithm, all of them are declared as permanent and they are never dealt with again.
	\item There is at most one edge leaving a component from a non-root vertex.
	The vertex $w$ connected with such an edge to a component with root $u$ is heavy.
	By contradiction, let it be a non-heavy vertex which is a root of a permanent component.
	In case (2) and (3) only permanent components involving a heavy vertex are created.
	In case (1) an undersized permanent component can be created, however a regular permanent component is created as well.
	The subtree size is therefore at least $B$, a contradiction.
	As a consequence, $u$ is heavy as it is an ancestor of $w$.
	
	Let's assume that it holds for all children, then it holds after the merge at vertex $v$ too.
	We are dealing with a temporary component of vertex $u$ and a permanent component of $w'$ which we want to connect to it.
	Both $u$ and $w'$ are heavy, so case (3) applies; the temporary component is not changed and immediately declared permanent.
	\item The number of component is $O\left(\frac{n}{B}\right)$.
	An undersized component was declared permanent in (1) and (2) only when another component of a regular size was declared permanent; in (3) when it was connected with a branching edge, or once per interval of non-heavy children which was either separated by a branching edge on the left or it is the first interval of a branching vertex or it is the branching vertex itself.
	From that the bound follows.
\end{itemize}

\begin{algorithmic}
\Function{Decompose}{$v, B$}
	\If{$is\_root(v)$}
		\State \Return{$\O, \{v\}$} \Comment{No permanent components, itself as temporary}
	\Else
		\State $P \gets \O; T \gets []; h \gets 0$
		\ForAll{$u \gets children(v)$}
			\State $(p, t) = Decompose(u)$ \Comment{Process child}
			\State $P \gets P \cup p; T.append(t)$ \Comment{Gather permanent and temporary}
			\If{$size(u) \ge B$} \Comment{Count number of heavy children}
				\State $h \gets h + 1$
			\EndIf
		\EndFor
		\State
		
		\State $s \gets \{v\}; i \gets 0$ \Comment{Working component, }
		\ForAll{$(u, t) \gets zip(children(v), T)$}
			\If{$size(u) \ge B \booland t \ne \O \booland h > 1$} \Comment{Heavy, temporary, case 3}
				\State $P \gets P \cup \{t\}; t \gets \O$ \Comment{Declare permanent}
			\EndIf
			\If{$t = \O \booland |s| > 1 \booland h > 1$} \Comment{Permanent, non-empty $s$, case 3}
				\State $P \gets P \cup \{s\}; s \gets \{v\}$  \Comment{Declare permanent, reset}
			\ElsIf{$t \ne \O$} \Comment{Temporary}
				\If{$|s| + |t| \ge B$} \Comment{Regular sized}
					\State $P \gets P \cup \{s \cup t\}; s \gets \{v\}; i \gets i + 1$ \Comment{Declare permanent}
				\Else
					\State $s \gets s \cup t$ \Comment{Merge $t$ into $s$}
				\EndIf
			\EndIf
		\EndFor
		\State
		
		\If{$|s| = 1$}
			\State $s \gets \O$ \Comment{Reset before returning}
		\ElsIf{$i \ge 1 \boolor h > 1$} \Comment{At least one regular sized or case 3}
			\State $P \gets P \cup \{s \cup t\}; s \gets \O$ \Comment{Declare permanent}
		\EndIf
		\If{$h = degree(v) \booland h > 1$} \Comment{All are heavy, case 3}
			\State $P \gets P \cup \{v\}$ \Comment{Declare $v$ as permanent}
		\EndIf
		
		\State \Return{$P, s$}
	\EndIf
\EndFunction
\end{algorithmic}

\subsection{The Structure}

We run the decomposition algorithm twice; the first time with $B = \log^c n$ for $c \ge 2$ which will be specified later.
The second time we decompose the components into small components with parameter $b = \frac{n}{8}$.
The components form connected subtrees in the tree, which we call mini-trees; similarly we call the subtrees in small components micro-trees.

There are three different connections between mini-trees (and similarly between micro-trees):
\begin{enumerate}
	\item two mini-trees share their root vertex;
	\item a parent of a root of a mini-tree is a root of a different mini-tree;
	\item a parent of a root of a mini-tree (bottom) is a non-root vertex in a different mini-tree (top);
\end{enumerate}

We define a set of terms referring to root structures, which we use to refer to them concisely.
A \emph{mini-tree root} is the root of a mini-tree.
A \emph{root mini-tree} is any mini-tree which contains the mini-tree root.
Similarly we define the terms for micro-trees.
We also use negations, for example ``a micro-tree root of a non-root mini-tree''.

We order the mini-trees based on their pre-order number in the original tree:
$$p, q: root(p) \le root(q) \booland child(root(p)) < child(root(q))$$
Each mini-tree is then assigned number $\tau_1$ based on this order.
Similarly we assign a numbers $\tau_2$ to each micro-tree in a mini-tree $\tau_1$ and name it $(\tau_1, \tau_2)$.
We continue in the same way with individual vertices; they are given a name $(\tau_1, \tau_2, \tau_3)$.

The size of $\tau_1$ is $O(\log n)$; the sizes of $\tau_2$ and $\tau_3$ are $O(\log \log n)$.
A vertex can get assigned multiple names when it is a common root of several micro-trees, which could be in different mini-trees.
A \emph{canonical name} of a mini-tree, micro-tree, or a vertex is the lexicographically smallest one.
All operations on the tree use (take and return) the canonical names of a vertices; non-canonical and non-full names are used only for internal purposes of the structure.

In order to handle the case (3), we alter the process of decomposition of the tree.
After the mini-trees are identified by the decomposition algorithm, we subdivide the edge making the connection in the case (3) by introducing a \emph{dummy vertex}, to which we refer as mini-tree dummy vertex.
This new vertex is then added to the top mini-tree, which changes its size.
As we add at most one vertex per mini-tree, the upper bound increases only to $2 B - 1$.
Then we follow with the second level decomposition and again introduce the dummy vertices, to which we refer as micro-tree dummy vertices.

The total number of dummy vertices introduced is $O\left(\frac{n}{B} + \frac{n}{b}\right) = O\left(\frac{n}{\log n}\right)$.
The dummy vertex gets its name as if it was a normal vertex in the top mini-tree or micro-tree.
The canonical name of a dummy vertex is the canonical name of the root to which it connects the top mini-tree or micro-tree.
The depth of a dummy vertex in a mini-tree or a micro-tree is at least $2$ because it has a parent which is not a root.

We call \emph{primary} the mini-tree $\tau_1$ and the micro-tree $(\tau_1, \tau_2)$ which contain a root with a canonical name $(\tau_1, \tau_2, 0)$.
We also call primary the micro-tree $(\tau_1, 0, 0)$ so that every micro-tree has its primary micro-tree within the same mini-tree.
A root micro-tree can have two primary micro-trees: one within the same mini-tree, the other one in its primary mini-tree.
In the latter case we add the phrase ``in the primary mini-tree''.

Sometime it is crucial to account every vertex once; we do so by introducing a relation \emph{to belong to}.
On a mini-tree level a non-root vertex belongs to its mini-tree; a mini-tree root belongs to the primary mini-tree.
On a micro-tree level a non-root vertex belongs to its micro-tree; a root which belongs to this mini-tree belongs to the primary micro-tree.
Each vertex of the tree belongs to exactly one mini-tree and one micro-tree.
The canonical name encodes the mini-tree and micro-tree to which a vertex belongs.

A vertex in a mini-tree or a micro-tree is called \emph{first} if is belongs to the mini-tree or micro-tree and has the smallest name.
We can find it by asking if the root belongs to the mini-tree or the micro-tree; if not, we set $\tau_3 = 1$.
Note that although there can be a micro-tree having a single vertex, the vertex is never shared with other micro-trees and therefore it belongs to it.

\subsubsection{Compressed Array}

We will often define an array $A$ which consists of $\tau_1$ (or $\tau_2$) names of a set of vertices in some particular order.
We call a consecutive subsequence of the same names a \emph{run}.
We compress this array by introducing three structures:
\begin{itemize}
	\item $A\_names$ -- an array of names like the the original $A$; all runs are replaced by a single value;
	\item $A\_prefix$ -- a prefix sum structure of numbers of occurrences of names in runs in the array $A$;
	\item $A\_run$ -- an array of numbers of how many times there was a run with such name before;
	\item $A\_before$ -- an array of numbers of how many time the same there was the names in previous runs.
\end{itemize}

Both arrays $A\_run$ and $A\_before$ contains the same number of elements as $A\_names$; size of their elements is also bounded by the size of element of $A\_names$.
Storing the information takes asymptotically less or the same space.

The composed operations available to this compressed array are:
\begin{itemize}
	\item querying a value of the array:
		$$A[i] = A\_names[prefix\_geq(A\_prefix, i)]$$
	\item finding the upper and lower borders of the run containing $i$: 
	\begin{align*}
		lower(i) &= prefix\_sum(A\_prefix, prefix\_geq(A\_prefix, i) - 1) \\
		upper(i) &= prefix\_sum(A\_prefix, prefix\_geq(A\_prefix, i))
	\end{align*} 
	\item finding the current number of repetitions of the run containing $i$:
	$$ run(i) = A\_run[prefix\_geq(A\_prefix, i)] $$
	\item finding how many $A[i]$ there were in $A$ before the position $i$:
	$$ before(i) = A\_before[prefix\_geq(A\_prefix, i)] + i - lower(i) $$
	\item finding the length of the array if it is not stored explicitly:
	$$ length() = prefix\_sum(A\_prefix, \infty)$$
\end{itemize}

We use this compressed array in two different settings:
\begin{itemize}
	\item A single array $A$ containing $a$ elements of size $s$ in $r$ runs.
	Then the size of $A\_names$ is $r s$ bits.
	The prefix structure can be stored using $\log {a \choose r} + o(r) + O(\log \log a) = O(r \log a)$ bits.
	
	Specifically, if we store $\tau_1$ names of all vertices of the tree requiring $O(1)$ runs per name, then the space is $O(\frac{n}{B} \log n) = o(n)$ bits.
	Equivalently for $\tau_2$ names of vertices of a given mini-tree it is $O(\frac{B}{b} \log b) = o(B)$ bits.
	\item A collection of $r$ arrays $A_i$, each containing up to $a$ elements of size $s$, with $r$ runs in total in all arrays of the collection.
	Then the arrays $A\_names$ require space $r s$ bits.
	The size of the $A\_prefix$ structures can be bounded using Jensen's inequality to $O(r \log a)$ bits.
	
	If each $\tau_1$ name occurs in at most one array in a single run, then partitioning all vertices of the tree requires $O(\frac{n}{B} \log n) = o(n)$ bits.
	Storing $\tau_2$ names of vertices in a given mini-tree under the same conditions gives the space $O(\frac{B}{b} \log B) = o(B)$ bits.
\end{itemize}
If $A$ contains all vertices of the tree (or a mini-tree) with at most $O(1)$ runs per name, 

The other use concerns multiple compressed arrays, with each vertex being present in $O(1)$ of them (it can even not appear at all), however every run of a name occurs at most once per compressed array.
For the whole tree with a compressed array per mini-tree, the arrays $A\_name$ add up to $O(\frac{n}{B})$ elements.
Jensen's inequality for the $A\_prefix$ gives us an upper bound of $O(\frac{n}{B} \log n) = o(n)$ bits in total.
The additional terms of the index dictionary $\sum_i (o(r_i) + O(\log \log n)) = O(\frac{n}{B} + \frac{n}{B} \log \log n) = o(n)$ also cause no problem.

Multiple compressed arrays on a micro-tree level, in the same settings provide the arrays $A\_name$ to have $O(\frac{B}{b})$ elements in total.
The $A\_prefix$ structures are bounded together by $O(\frac{B}{b} \log B) = o(B)$ with the additional terms $O(\frac{B}{b} + \frac{B}{b} \log \log B) = o(B)$.

\subsubsection{Representation}

For each micro-tree we store several pieces of information:
\begin{enumerate}
	\item Succinct representation of the micro-tree together with its size.
	\begin{itemize}
		\item $size$ -- size $k$ of the micro-tree;
		\item $rep$ -- its succinct (or even implicit) representation of size $2k$ bits.
	\end{itemize}
	
	\item Identity of the micro-tree:
	\begin{itemize}
		\item $\tau_2$ -- its $\tau_2$ name; if the name equals to $0$, it is a root micro-tree;
		\item $offset$ -- the offset from the start of the representation of the mini-tree in bits.
		While $\tau_1$ name is too big, the offset is at most $B + o(B)$, which results in $\log \log n$ bits.
		We subtract the offset from the beginning of the micro-tree structure to get to the mini-tree structure.
		\item $primary_2$ -- $\tau_2$ name of the primary micro-tree; it can be the micro-tree itself.
	\end{itemize}

	\item The parent and dummy vertex handling:
	\begin{itemize}
		\item $parent_2$ -- $\tau_2$ name of the micro-tree containing its parent.
		The parent could in theory be stored only in the primary micro-tree, however, it is small enough to keep a copy in all micro-trees.
		\item $type3$ -- a single bit denoting whether it is connected to its parent by a type (3) connection
		\item $dummy_2$ -- $\tau_3$ name of the dummy node which represents connection to a different micro-tree.
		The lack of it means that the micro-tree does not have a dummy vertex.
		\item $bottom_2$ -- $\tau_2$ name of the micro-tree to which the dummy vertex leads.
		The lack of the name is interpreted as that it leads to a different mini-tree, which is handled on the mini-tree level.
		In such case it is the dummy vertex introduced after the first run of the decomposition algorithm.
		$bottom$ is always the primary micro-tree.
	\end{itemize}
	
	\item Handling children:
	\begin{itemize}
		\item $C_2$ -- a compressed array of $\tau_2$ names of children of the root restricted to the current mini-tree.
		It is stored only in the primary micro-tree or micro-tree with $\tau_2 = 0$.
		\item $C_2\_index$ -- the index of $\tau_2$ within the array $C_2\_names$ stored in the primary micro-tree;
		\item $C_2\_parent$ -- the index of $\tau_2$ within the array $C_2\_names$ stored in the parent's micro-tree.
		This index is null if $type3 = 1$ because in such case the micro-tree root is not a child of parent's micro-tree root.
	\end{itemize}
\end{enumerate}

The size in (1) and all fields in (2) and (3) and the indices in (4) occupy only $O(\log \log n)$ bits per micro-tree.
We have already analyzed the $C_2$ structures, which yields $o(B)$ bits per mini-tree.
All succinct representations of micro-trees contain all $n$ vertices of the tree together with $o(n)$ dummy vertices and $o(n)$ shared root vertices.
The representations requires $2n + o(n)$ bits and are the dominant part of the structure.

Each micro-tree contains $k$ vertices, which has an upper bound of $ k \le 2b - 1 < \frac{\log n}{4}$ vertices.
We can represent this subtree succinctly using $2k$ bits.
The representation takes less than $\frac{n}{2}$ bits, which is small enough to use it as an index to a precomputed look-up table.
All our look-up tables require space $o(n)$.

\bigskip

We represent each mini-tree with a structure containing the following information:
\begin{enumerate}
	\item Identity of the mini-tree:
	\begin{itemize}
		\item $\tau_1$ -- its $\tau_1$ name;
		\item $micro\_trees$ -- an array of offsets of all its micro-trees.
		\item $primary_1$ -- $\tau_1$ name of the primary mini-tree; it can be the mini-tree itself.
	\end{itemize}

	\item The parent and the dummy vertex handling:
	\begin{itemize}
		\item $parent_1$ -- $\tau_1$ name of the mini-tree which contains the parent of the root of this mini-tree.
		\item $dummy_1$ -- $\tau_2$ name of the micro-tree which contains the dummy vertex which was introduced for edges between mini-trees.
		It can be null, if no dummy vertex exist in the mini-tree.
		\item $bottom_1$ -- $\tau_1$ name of the mini-tree where the dummy vertex leads.
	\end{itemize}
	
	\item The children of the mini-tree root:
	\begin{itemize}
		\item $C_1$ -- a compressed array of $\tau_1$ names of children of the mini-tree root.
		It is stored only in the primary mini-tree.
		\item $C_1\_index$ -- the index of $\tau_1$ within the array $C_1\_names$ stored in the primary mini-tree;
		\item $C_1\_parent$ -- the index of $\tau_1$ within the array $C_1\_names$ stored in the parent's mini-tree.
		This index is null if $type3 = 1$ because in such case the mini-tree root is not a child of parent's mini-tree root.
	\end{itemize}
	
	\item Micro-trees stored consecutively in memory.
\end{enumerate}

The names in (1) and (2) and the indices in (3) require $O(\log n)$ bits per mini-tree.
We have already analyzed the $C_1$ structures, which yields $o(n)$ bits overall.
The array of offsets in (1) has size $O\left(\frac{B}{b} \log (B + o(B))\right) = o(B)$ bits per mini-tree.

\bigskip

At the global level, we only need to a very simple structure:
\begin{itemize}
	\item $mini\_trees$ -- an array of offsets of all its mini-trees.
	\item Mini-trees stored consecutively in memory.
\end{itemize}

The array of offsets has size of $O(\frac{n}{B} \log (n + o(n))) = o(n)$ bits.

\subsection{Basic Navigation}

With the structure we have described, the basic navigation can already be supported.
In the algorithms we identify the structures with their names provided that we can navigate to them.
We can navigate to a mini-tree structure $\tau_1$ name using the array $mini\_trees$ and to a micro-tree structure $(\tau_1, \tau_2)$ using the array $micro\_trees$.
We also assume that the global fields and the fields of the mini-tree and immediately accessible from a micro-tree.

We define three helper functions which traverse the edge of type (3) and canonize the name of a vertex.
As they are not part of the interface, the $i$ can be a non-canonical name.

\begin{algorithmic}
\Function{dummy\_up}{$i$}
	\If{$i.\tau_3 = 0 \booland i.type3$}
		\If{$i.\tau_2 \ne 0$}
			\State \Return{$(i.\tau_1, parent_2, parent_2.dummy_2)$}
		\Else
			\State \Return{$(i.parent_1, i.parent_2, i.parent_1[parent_2].dummy_2)$}
		\EndIf
	\Else
		\State \Return{$i$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{dummy\_down}{$i$}
	\If{$i.\tau_3 = i.dummy_2$}
		\If{$i.bottom_2 \ne null$}
			\State \Return{$(i.\tau_1, i.bottom_2, 0)$}
		\Else
			\State \Return{$(i.bottom_1, i.bottom_2, 0)$}
		\EndIf
	\Else
		\State \Return{$i$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{canonize}{$i$}
	\If{$i.\tau_3 \ne 0$}
		\State \Return{$dummy\_down(i)$}
	\Else
		\State $j \gets (i.\tau_1, i.primary_2, 0)$
		\If{$j.\tau_2 \ne 0$}
			\State \Return{$j$}
		\Else
			\State \Return{$(j.primary_1, 0, 0)$}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

We add four more functions which navigate to special vertices in a micro-tree or a mini-tree.

\begin{algorithmic}
\Function{root\_1}{$i$}
	\State \Return{$canonize(i.\tau_1, 0, 0)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{root\_2}{$i$}
	\State \Return{$canonize(i.\tau_1, i.\tau_2, 0)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{dummy\_1}{$i$} \Comment{Assuming that a mini-tree dummy vertex exists}
	\State \Return{$(i.\tau_1, i.dummy_1, i.dummy_1.dummy_2)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{dummy\_2}{$i$} \Comment{Assuming that a micro-tree dummy vertex exists}
	\State \Return{$(i.\tau_1, i.\tau_2, i.dummy_2)$}
\EndFunction
\end{algorithmic}

For the function $is\_root$ and $is\_leaf$, $i$ is a canonical name, which we use a lot:
If $i$ is a root of the tree, it is part of the primary root mini-tree, which in pre-order numbering has the number $0$.
We apply the same reasoning on micro-micro trees inside the mini-tree and to the vertex inside the micro-tree.

Similarly, because of canonicity, $i$ cannot encode the name of a dummy vertex inside the top micro-tree.
The $degree$ table use in $is\_leaf$ can therefore be oblivious of existence of dummy vertices.

\begin{algorithmic}
\Function{is\_root}{$i$}
	\State \Return{$i.\tau_1 = 0 \booland i.\tau_2 = 0 \booland i.\tau_3 = 0$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{is\_leaf}{$i$}
	\State \Return{$degree[i.size, i.rep, i.\tau_3] = 0$}
\EndFunction
\end{algorithmic}

We assume that there is a field $first_2$ which contains $\tau_3$ name of the first vertex; this can either be stored using a single bit or computed:
$$first_2(i) = (i.\tau_2 = i.primary_2 \booland (i.primary_2 \ne 0 \boolor i.\tau_1 = i.primary_1)$$
Similarly for the mini-tree first vertex: $first_1(i) = (\tau_1, 0, 0).first_2$.

\subsubsection{Parent}

A vertex can potentially use a connection of type (3) to its parent; we handle that by the $dummy_up$ function.
Then we proceed differently in cases of non-root, micro-tree root but not mini-tree root, and mini-tree root.

If the current vertex is not a micro-tree root ($\tau_3 \ne 0$), we use a look-up table to get the answer.
If it is the micro-tree root ($\tau_3 = 0$) but not a mini-tree root ($\tau_2 \ne 0$), then the parent is present in a micro-tree $parent_2$ within the same mini-tree.
If the vertex is a mini-tree root, but not the root of the whole tree ($\tau_2 = 0, \tau_1 \ne 0$), the answer is the root of a parent mini-tree.
The last case is a parent of the root vertex $(0, 0, 0)$ which simply does not exist.

\begin{algorithmic}
\Function{parent}{$i$}
	\State $i \gets dummy_up(i)$
	\If{$i.\tau_3 \ne 0$}
		\State \Return{$(i.\tau_1, i.\tau_2, parent[i.size, i.rep, i.\tau_3])$}
	\ElsIf{$i.\tau_2 \ne 0$}
		\State \Return{$(i.\tau_1, i.parent_2, 0)$}
	\ElsIf{$i.\tau_1 \ne 0$}
		\State \Return{$(i.parent_1, 0, 0)$}
	\Else
		\State \Return{$-1$}
	\EndIf
\EndFunction
\end{algorithmic}

\subsubsection{Children}

We solve the operations on children (degree, rank and select) using the compressed arrays $C_1$ and $C_2$, while handling dummy vertices.

\begin{algorithmic}
\Function{degree}{$i$}
	\If{$i.\tau_3 \ne 0$}
		\State \Return{$degree[i.size, i.rep, i.\tau_3]$}
	\ElsIf{$i.\tau_2 \ne 0$}
		\State \Return{$length(i.C_2\_prefix, \infty)$}
	\Else
		\State \Return{$lebgth(i.C_1\_prefix, \infty)$}
	\EndIf
\EndFunction
\end{algorithmic}

If parent of $i$ is non-root, a look-up table solves that case.
Otherwise we identify where the current micro-tree is located in parent's $C_2$ sequence (and $C_1$ for a mini-tree root) following by finding the $child_rank$ within the micro-tree.

\begin{algorithmic}
\Function{child\_rank}{$i$}
	\State $p \gets parent(i)$
	\State $i \gets dummy\_up(i)$
	\If{$p.\tau_3 \ne 0$}
		\State \Return{$child\_rank[p.size, p.rep, i.\tau_3]$}
	\ElsIf{$p.\tau_2 \ne 0$}
		\If{$i.primary_2 = p.\tau_2$} \Comment{Connection type (1) between micro-trees}
			\State $l \gets prefix\_sum(p.C_2\_prefix, i.C_2\_index - 1)$
			\State \Return{$l + child\_rank[i.size, i.dep, i.\tau_3]$}
		\Else \Comment{Connection type (2) between micro-trees}
			\State $l \gets prefix\_sum(p.C_2\_prefix, i.C_2\_parent - 1)$
			\State \Return{$l + 1$}
		\EndIf
	\ElsIf{$p.\tau_1 \ne 0$}
		\If{$i.primary_1 = p.\tau_1$} \Comment{Connection type (1) between mini-trees}
			\State $l_1 \gets prefix\_sum(p.C_1\_prefix, i.C_1\_index - 1)$
			\If{$i.primary_2 = p.\tau_2$} \Comment{Connection type (1) between micro-trees}
				\State $l_2 \gets prefix\_sum(p.C_2\_prefix, i.C_2\_index - 1)$
				\State \Return{$l_1 + l_2 + child\_rank[i.size, i.dep, i.\tau_3]$}
			\Else \Comment{Connection type (2) between micro-trees}
				\State $l_2 \gets prefix\_sum(p.C_2\_prefix, i.C_2\_parent - 1)$
				\State \Return{$l_1 + l_2 + 1$}
			\EndIf
		\Else \Comment{Connection type (2) between mini-trees}
			\State $l \gets prefix\_sum(p.C_1\_prefix, i.C_1\_parent - 1)$
			\State \Return{$l + 1$}
		\EndIf
	\Else
		\State \Return{$-1$}
	\EndIf
\EndFunction
\end{algorithmic}

For selecting the child, we start with the compressed arrays to find in which micro-tree is the child present and we finish the operation with a table look-up.

\begin{algorithmic}
\Function{child\_select}{$i, n$}
	\If{$i.\tau_3 \ne 0$}
		\State $d \gets child\_select[size, rep, i.\tau_3, n]$
		\State \Return{$dummy\_down(d)$}
	\ElsIf{$i.\tau_2 \ne 0$}
		\State $d \gets i.C_2[n - 1]$
		\State $n' \gets before(i.C_2, n - 1) + 1$
		\If{$d.primary_2 = i.\tau_2$} \Comment{Connection type (1) between micro-trees}
			\State \Return{$child\_select[d.size, d.rep, 0, n']$}
		\Else \Comment{Connection type (2) between micro-trees}
			\State \Return{$(i.\tau_1, d, 0)$}
		\EndIf
	\Else
		\State $d_1 \gets i.C_1[n - 1][0]$ \Comment{Micro-tree $0$ in the correct mini-tree}
		\State $n' \gets before(i.C_1, n - 1) + 1$
		\If{$d_1.primary_1 = i.\tau_1$} \Comment{Connection type (1) between mini-trees}
			\State $d_2 \gets d_1.C_2[n' - 1]$
			\State $n'' \gets before(d_1.C_2, n' - 1) + 1$
			\If{$d_2.primary_2 = i.\tau_2$} \Comment{Connection type (1) between micro-trees}
				\State \Return{$child\_select[d_2.size, d_2.rep, 0, n'']$}
			\Else \Comment{Connection type (2) between micro-trees}
				\State \Return{$(d_2.\tau_1, d_2, 0)$}
			\EndIf
		\Else \Comment{Connection type (2) between mini-trees}
			\State \Return{$(d_1.\tau_1, d_1, 0)$}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

\bigskip

In order to support more operations we need to augment the structure with more information.

\subsection{Depth, Deepest Vertex, Height, Subtree-Size}

\begin{enumerate}
	\item Micro-tree structure for micro-trees:
	\begin{itemize}
		\item $depth_2$ -- depth of the root within the mini-tree;
		\item $dummy\_ancestor$ -- one bit flag if the micro-tree is an ancestor of a dummy vertex connecting a mini-tree;
		\item $subtree\_size_2$ -- size of the subtree excluding the any mini-tree connected via a dummy vertex;
		\item $deepest_2$ -- $(\tau_2, \tau_3)$ name of the deepest vertex within the subtree restricted to the current mini-tree.
		It often is the dummy vertex connecting the micro-tree to another mini-tree.
	\end{itemize}
	\item Mini-tree structure contains information for root micro-trees:
	\begin{itemize}
		\item $depth_1$ -- depth of the mini-tree root within the tree;
		\item $subtree\_size_1$ -- size of the subtree of the mini-tree root;
		\item $deepest_1$ -- the full name of the deepest vertex within the subtree.
	\end{itemize}
\end{enumerate}

The depth operation is straight forward; it adds the micro-tree-local depth of the vertex, mini-tree-local depth of a micro-tree root and global depth of a mini-tree root.

\begin{algorithmic}
\Function{depth}{$i$}
	\If{$i.\tau_3 \ne 0$}
		\State \Return{$depth(i.\tau_1, i.\tau_2, 0) + depth[i.size, i.rep, i.\tau_3]$}
	\ElsIf{$i.\tau_2 \ne 0$}
		\State \Return{$depth(i.\tau_1, 0, 0) + i.depth_2$}
	\Else
		\State \Return{$i.depth_1$}
	\EndIf
\EndFunction
\end{algorithmic}

The operation returning the deepest vertex is used also for computing height of a vertex.
If $i$ is not a root of a micro-tree, we find the deepest vertex using a look-up table.
We also consider the alternative that the deepest vertex lays inside a micro-tree or a mini-tree connected via a dummy vertex if $i$ is its ancestor.
We will never look for a deepest vertex for a non-root argument.

If $i$ is a root of a micro-tree but not of a mini-tree, we answer with $deepest_2$ if it is not a dummy vertex, otherwise we follow the connection to a mini-tree root.
If $i$ is a root of a mini-tree, we answer with immediately with $deepest_1$.

\begin{algorithmic}
\Function{deepest\_vertex}{$i$}
	\If{$i.\tau_3 \ne 0$}
		\State $d \gets (i.\tau_1, i.\tau_2, deepest[i.size, i.rep, i.\tau_3])$
		\If{$is\_ancestor[i.size, i.rep, i.\tau_3, i.dummy_2]$}
			\State $d' \gets deepest\_vertex(dummy\_down(i.\tau_1, i.\tau_2, i.dummy_2))$
			\If{$d.\tau_3 = i.dummy_2$}
				\State \Return{$d'$}
			\Else
				\State \Return{$depth(d) > depth(d') \mathbin{?} d : d'$}
			\EndIf
		\Else
			\State \Return{$d$}
		\EndIf
	\ElsIf{$i.\tau_2 \ne 0$}
		\State $d \gets (i.\tau_1, i.deepest_2.\tau_2, i.deepest_2.\tau_3)$
		\If{$d.\tau_3 = d.dummy_2$}
			\State \Return{$deepest\_vertex(dummy\_down(d))$}
		\Else
			\Return{$d$}
		\EndIf
	\Else
		\State \Return{$i.deepest_1$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{height}{$i$}
	\State $d \gets deepest\_vertex(i)$
	\State \Return{$depth(d) - depth(i)$}
\EndFunction
\end{algorithmic}

The operation $subtree\_size$ works similarly to the previous ones.
The answer is known for a mini-tree root.
In case of a micro-tree root we know the subtree size within the mini-tree; we must add the subtree size of the mini-tree which can be connected to a dummy vertex in the subtree, for which we use the flag $dummy\_ancestor$.
Finally, in case of a non-root, we use the look-up table plus we add the size of anything connected by a dummy vertex, and we subtract $1$ for the dummy vertex.

\begin{algorithmic}
\Function{subtree\_size}{$i$}
	\If{$i.\tau_3 \ne 0$}
		\State $ss \gets subtree\_size[i.size, i.rep, i.\tau_3]$
		\If{$is\_ancestor[i.size, i.rep, i.\tau_3, i.dummy_2]$}
			\State \Return{$ss - 1 + subtree\_size(dummy\_down(i.\tau_1, i.\tau_2, i.dummy_2))$}
		\Else
			\State \Return{$ss$}
		\EndIf
	\ElsIf{$i.\tau_2 \ne 0$}
		\If{$i.dummy\_ancestor$}
			\State \Return{$i.subtree\_size_2 + subtree\_size(i.bottom_1, 0, 0)$}
		\Else
			\State \Return{$i.subtree\_size_2$}
		\EndIf
	\Else
		\State \Return{$i.subtree\_size_1$}
	\EndIf
\EndFunction
\end{algorithmic}

\subsection{Vertex and Leaf Ranks and Selects}

We again augment the global structure and the structures of mini-trees and micro-trees with additional data to support new operations.
We consider four different vertex numbering schema and depending on it, we store different kinds of information.

\subsubsection{Pre-Order Numbering}

We store the explicit pre-order number $pre\_first_1$ for the first vertex belonging to a mini-tree in the mini-tree structure.
We store the offset of the pre-order number $pre\_first_2$ for the first vertex belonging to a micro-tree in the micro-tree structure skipping the mini-tree connected via the type (3) connection.

In the rank algorithm, when we count the rank of a root of either mini-tree or a micro-tree, we know that the root of the micro-tree is its first vertex from canonicity of names.
In case of a rank of a micro-tree root but not a mini-tree root, we need to add the count of type (3) connected mini-trees if it is before the current micro-tree.
If we count the rank of a non-root vertex, we determine its offset from the first vertex and then add the offset of the vertex within the micro-tree compensating possibly for a non-root first vertex and a dummy vertex.

\begin{algorithmic}
\Function{pre\_rank}{$i$}
	\If{$i.\tau_3 \ne 0$}
		\Comment{We determine the rank of the first vertex}
		\If{$i.dummy_1 \ne null \booland i.dummy_1.pre\_first_2 < i.pre\_first_2$}
			\State $d_1 \gets dummy\_down(i.\tau_1, i.dummy_1, i.dummy_1.dummy_2)$
			\State $f \gets i.pre\_first_1 + i.pre\_first_2 + subtree\_size(d_1)$
		\Else
			\State $f \gets i.pre\_first_1 + i.pre\_first_2$
		\EndIf
		
		\State $r \gets pre\_rank[i.size, i.rep, i.\tau_3]$
		\If{$i.dummy_2 \ne null \booland pre\_rank[i.size, i.rep, i.dummy_2] < r$}
			\State $d_2 \gets dummy\_down(i.\tau_1, i.\tau_2, i.dummy_2)$
			\State \Return{$f + r - first_2(i) - subtree\_size(d_2) - 1$}
		\Else
			\State \Return{$f + r - first_2(i)$}
		\EndIf
	\ElsIf{$i.\tau_2 \ne 0$}
		\If{$i.dummy_1 \ne null \booland i.dummy_1.pre\_first_2 < i.pre\_first_2$}
			\State $d \gets dummy\_down(i.\tau_1, i.dummy_1, i.dummy_1.dummy_2)$
			\State \Return{$i.pre\_first_1 + i.pre\_first_2 + subtree\_size(d)$}
		\Else
			\State \Return{$i.pre\_first_1 + i.pre\_first_2$}
		\EndIf
	\Else
		\State \Return{$i.pre\_first_1$}
	\EndIf
\EndFunction
\end{algorithmic}

We store a compressed array of $\tau_1$ names of all vertices in the tree in pre-order; this is $o(n)$ bits as we have shown before.
Similarly we have a compressed array for each mini-tree containing $\tau_2$ names of vertices belonging to the mini-tree in pre-order.
The compressed array $\tau_2$ requires $o(B)$ bits per mini-tree.

We use similar technique as for $child\_select$, however this time we deal with one name appearing in up to two runs, which stems from type (3) connections which break the sequence into two.
We find the correct mini-tree and correct micro-tree, we can ignore type (3) connections as they were taken care of implicitly by the compressed arrays.
We use a look-up table in the micro-tree with correction for the non-root first vertex and the dummy vertex.

\begin{algorithmic}
\Function{pre\_select}{$n$}
	\State $d_1 \gets pre_1[n - 1]$ \Comment{Name of the mini-tree}
	\State $n' \gets before(pre_1, n - 1) + 1$

	\State $d_2 \gets d_1.pre_2[n' - 1]$ \Comment{Name of the micro-tree}
	\State $n'' \gets before(d_1.pre_2, n' - 1) + 1$
	
	\State{$r \gets n'' + (1 - first_2(d_2))$} \Comment{Correction for non-root}
	\State{$r' \gets r + (d_2.dummy_2 \ne null \booland pre\_rank[d_2.size, d_2.rep, d_2.dummy_2] < r)$}
	\State \Return{$(d_1, d_2, pre\_select[d_2.size, d_2.rep, r'])$}
\EndFunction
\end{algorithmic}

\paragraph{Ancestor Checking}

We can use pre-order rank to check whether a vertex is an ancestor of another vertex.

\begin{algorithmic}
\Function{is\_ancestor}{$i, j$}
	\State \Return{$pre\_rank(i) \le pre\_rank(j) \le pre\_rank(i) + subtree\_size(i) - 1$}
\EndFunction
\end{algorithmic}

\subsubsection{Post-Order Numbering}

The post-order rank and select work in a very similar way as their pre-order variants.
There are several differences which make it more complicated.
We redefine what belonging means -- a vertex belongs to the mini-tree and micro-tree with the biggest name instead of the smallest one.
We introduce an analogy to primary fields called $terminary_1$ and $terminary_2$.
The correction because of the ``first'' vertex is not necessary as the root is assigned the rank as the last vertex of the micro-tree.

We omit the algorithm here because it is only adds an indirection by jumps to terminary mini-trees and micro-trees while the gain from it is negligible.

\subsubsection{DFUDS-Order Numbering}

The DFUDS-order number of a vertex can be determined recursively:
\todo{move to definition of ranks, make it a lemma}

\begin{align*}
DFUDS\_rank(i) &=
\begin{cases}
	1 & is\_root(i) \\
	pre\_rank(i) + DFUDS\_anc(i)  & child\_rank(i) = 1 \\
	\begin{aligned}
		&DFUDS\_rank(child\_first(parent(i))) \\
		&\ + child\_rank(i) - 1
	\end{aligned} & \textrm{otherwise} \\
\end{cases} \\
DFUDS\_anc(i) &= \sum_{a \in anc(i) \setminus \{i\}} |right\_siblings(a)|
\end{align*}


Finding the DFUDS-order number of a non-first child is easy and does not require any special consideration.
In case of the first child, we already know its pre-order number and it leaves to precompute the sum of right siblings of all ancestors.
In mini-tree roots we store the absolute sum $arss_1$; in micro-tree we store the sum $arss_2$ going up to mini-tree root except for it.
The right siblings of mini-tree root and micro-tree root are calculated when needed instead of being part of the precomputed values.

\begin{algorithmic}
\Function{DFUDS\_rank}{$i$}
	\If{$is\_root(i)$}
		\State \Return{$1$}
	\ElsIf{$child\_rank(i) > 1$}
		\State \Return{$DFUDS\_rank(child\_first(parent(i))) + child\_rank(i) - 1$}
	\Else
		\State $r \gets pre\_rank(i)$
		\State $root_1 \gets (i.primary_1, 0, 0)$ \Comment{Mini-tree root}
		\State $rs_1 \gets degree(parent(root_1)) - child\_rank(root_1)$ \Comment{Its right siblings}
		\If{$i.\tau_3 \ne 0$}
			\State $root_2 \gets (i.\tau_1, i.primary_2, 0)$ \Comment{Micro-tree root}
			\State $rs_2 \gets degree(parent(root_2)) - child\_rank(root_2)$
			\State $arss_3 \gets right\_siblings\_sum[i.size, i.rep, i.\tau_3]$
			\State \Return{$r + i.arss_1 + rs_1 + i.arss_2 + rs_2 + arrs_3$}
		\ElsIf{$i.\tau_2 \ne 0$}
			\State \Return{$r + i.arss_1 + rs_1 + i.arss_2$}
		\Else
			\State \Return{$r + i.arss_1$}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

For the DFUDS\_select operation, we store a similar compressed array $DFUDS_1$ as in pre-order case: the $\tau_1$ names of the vertices of the tree in DFUDS-order.
\begin{lemma}
	There can be up to four runs of a $\tau_1$ name in the array.
\end{lemma}
\begin{proof}
	For all vertices $v$ of a mini-tree except for a constant number of exceptions, it holds that the immediately preceding vertex in the array has the same $\tau_1$ name.
	The exceptions are:
	\begin{enumerate}
		\item the mini-tree root -- it is the first vertex in DFUDS-order;
		\item the $(\tau_1, 0, 1)$ -- unless the root was the only child of its parent;
		\item the vertex after the mini-tree dummy vertex -- the dummy vertex is canonized and the $\tau_1$ name of the connected mini-tree is used instead;
		\item the vertex after last vertex of the connected mini-tree via a dummy vertex.
	\end{enumerate}
	Each of these vertices start a run, therefore there are up to four runs.
\end{proof}
We compress the array, which results in only a constant times more space than in the pre-order case; it still uses $o(n)$ bits of memory.
We do the same on the mini-tree level with $DFUDS_2$.

$DFUDS\_select$ then works the same way as $pre\_select$, including the corrections in micro-trees.

\subsubsection{In-Order Numbering}

In-order number is assigned to a vertex which has a degree greater than $1$; we can therefore ignore vertices of degree $0$ or $1$.
A vertex is given its in-order number after its first subtree was processed.
We can establish a formula similar to DFUDS-order ranks:
\begin{align*}
	in\_rank(i) &= in\_anc(i) + in\_size(child\_first(i)) + 1 \\
	in\_anc(i) &= \sum_{a \in anc(i)} \sum_{l \in left\_siblings(a)} (1 + in\_size(l))
\end{align*}

We calculate how many in-order numbers have been assigned until $i$.
The $+1$ in $in\_anc$ is for in-order numbers already assigned to $parent(a)$.
We use a function $in\_size(i)$ which returns this number for a subtree of $i$.
This function is implemented the same way as $subtree\_size$ is.

Let $v_1 \in anc(i) \booland parent(v_1) = (i.\tau_1, 0, 0)$ and similarly $v_2 \in anc(i) \booland parent(v_2) = (i.\tau_1, i.\tau_2, 0)$.
We split the precomputed $in\_anc(i)$ into several parts (in the most general case):
\begin{alignat}{2}
	in\_anc(i) &=  in\_anc(parent(v_1)) \tag{A}\\
	&\qquad + in\_anc(v_1) &&- in\_anc(parent(v_1)) \tag{B}\\
	&\qquad + in\_anc(parent(v_2)) &&- in\_anc(v_1) \tag{C}\\
	&\qquad + in\_anc(v_2) &&- in\_anc(parent(v_2)) \tag{D}\\
	&\qquad + in\_anc(i) &&- in\_anc(v_2) \tag{E}
\end{alignat}

\begin{itemize}
	\item $A$ is stored in a mini-tree structure.
	\item $B$ is further split into $B_1$ and $B_2$:
	\begin{align*}
		B &= B_1 + B_2\\
		B_1 &= \sum_{\substack{l \in left\_siblings(v_1) \\ l.\tau_1 \ne v_1.\tau_1}} (1 + in\_size(l)) \\
		B_2 &= \sum_{\substack{l \in left\_siblings(v_1) \\ l.\tau_1 = v_1.\tau_1 \\ l.\tau_2 \ne v_1.\tau_2}} (1 + in\_size(l))
	\end{align*}
	$B_1$ is stored in a mini-tree structure, while $B_2$ is stored in a micro-tree structure.
	\item $C$ is stored in a micro-tree structure.
	\item $D$ is split like $B$ was:
	\begin{align*}
		D &= D_2 + D_3\\
		D_2 &= \sum_{\substack{l \in left\_siblings(v_2) \\ l.\tau_1 = v_2.\tau_1 \\ l.\tau_2 \ne v_2.\tau_2}} (1 + in\_size(l)) \\
		D_3 &= \sum_{\substack{l \in left\_siblings(v_2) \\ l.\tau_1 = v_2.\tau_1 \\ l.\tau_2 = v_2.\tau_2}} (1 + in\_size(l))
	\end{align*}
	$D_2$ is stored in a micro-tree structure, while $D_3$ is handled by a look-up table.
	\item $E$ is stored handled by a look-up table.
\end{itemize}

None of $B_2, C, D_2$ accounts for a mini-tree connected by a connection of type (3) because the fields would be too big for a micro-tree structure.
It is added later as we can detect such case: $pre\_rank(i.dummy_1) < pre\_rank(i)$.
$D_2$ and $E$ do not contain any type (3) connection; it can added later if $pre\_rank(i.dummy_2) < pre\_rank(i)$.

There are several special cases which need to be addressed:
\begin{itemize}
	\item If $i$ is a mini-tree root, $in\_anc(i) = A$.
	\item If $i$ is a micro-tree root, we omit the terms $D_2, D_3, E$.
	\item If $v_1 = v_2$ ($i$ is in a root micro-tree), we omit the terms $C$ and $D_2$.
	\item If $i = v_2$, we omit the term $E$.
\end{itemize}

This completes the rank.

The $in\_select$ is similar to $pre\_select$; it has two levels of compressed arrays which navigate to the micro-tree.
The array $in_1$ contains $\tau_1$ names of all vertices of the tree which have been assigned an in-order number in all their instances, therefore vertex $v$ appears $degree(v) - 1$ times.
We show that the number of runs is bounded by $O(\frac{n}{B})$.
The only a few cases when a vertex $v$ has a different $\tau_1$ name than its immediately preceding vertex $u$ in the array $in_1$.
\begin{itemize}
	\item $v$ is the first vertex in its mini-tree which is assigned an in-order number.
	This happens at most once per mini-tree.
	\item subtree of $v$ contains a mini-tree dummy vertex which leads to a subtree containing $u$.
	There are $O(\frac{n}{B})$ mini-tree dummy vertices in total.
	\item $v$ is a root and a different mini-tree $t$ containing $u$ in its subtree is connected to $v$ by connection of type (2).
	Once $t$ is entered, it is fully exhausted before vertex $v$ is visited again; $t$ can split the sequence of $v.\tau_1$ names only once.
	Every mini-tree is connected to a root in a different mini-tree at most once; this situation occurs $(\frac{n}{B})$ times in total.
	\item $v$ is a shared root.
	Then $v$ delimits the sequences of its children's subtrees, of which there can be $O(n)$, which is too much.
	As it is a shared root, we can choose which $\tau_1$ name of the mini-trees containing $v$ we put in the $in_1$ array.
	We choose the name of the mini-tree $t$ from which we have returned to $v$.
	This way the sequence of names just before returning to $v$ from $t$ and just after diving back to the $t$ is uninterrupted.
	This case does not add any runs because once we switch $v$ to $t$, $v.\tau_1$ is never used any more.
\end{itemize}
As the number of runs is bounded by $O(\frac{n}{B})$, the size required to store such array is $o(n)$ bits.

Using the compressed array $in_1$ we can get to the mini-tree which contains the vertex with the in-order number $n$, however the vertex might not belong to the mini-tree.
It can happen that $\tau_1$ was used in $in_1$ in one of the following corner cases, which are not handled by the array $in_2$ but can be answered directly.
\begin{itemize}
	\item If the mini-tree has the only one vertex, we simply return its name.
	We can therefore assume that $v$ has some children in the same mini-tree.
	\item If the mini-tree is the first one of those with which it is connected by type (1), then the first $l$ occurrences of $\tau_1$ in $in_1$ are due to $l$ type (2) connections to the left from mini-tree.
	We keep the value of $l$ precomputed in $ltype2_1$, if the condition does not apply, we assign it $0$.
	\item If the mini-tree has a connection of type (1) to a mini-tree to the right, then the last occurrence of $\tau_1$ is due to the transition between the mini-trees.
	\item If the mini-tree has $r$ type (2) connections to right from the mini-tree before $v$ is switched to another type (1) connected mini-tree,
	then the last $r$ occurrences are due to them.
\end{itemize}

The array $in_2$ contains only instances of in-order numbers which were assigned as a result of returning from and diving into vertices in the mini-tree.
The length of $in_2$ array is bounded by number of vertices in the mini-tree because a return from a vertex to its parent can cause its parent being assigned an in-order number.
By the same reasoning as $in_1$, the array $in_2$ contains only $O(\frac{B}{b})$ runs and requires $o(B)$ bits of space.
The same corner cases can happen on mini-tree level and they can be solved using a precomputed value $ltype2_2$ and $in\_size$.

Using the $in_1$ and $in_2$ compressed arrays, we can navigate to the correct micro-tree.
The $n''$ then denotes in-order number of a vertex in the micro-tree, which can be easily found using a look-up table.
No correction for the root nor dummy vertex is necessary.

\begin{algorithmic}
\Function{in\_select}{$n$}
	\State $d_1 \gets in_1[n - 1]$
	\State $n' \gets below(in_1, n - 1) + 1$
	\If{$d_1[0].size = 1$} \Comment{A single vertex mini-tree}
		\State \Return{$(d_1, 0, 0)$}
	\ElsIf{$n' \le d_1.ltype2_1$} \Comment{Left type (2) connections}
		\State \Return{$(d_1, 0, 0)$}
	\Else
		\State $n' \gets n' - d_1.ltype2_1$
		\If{$n' > length(d_1.in_2)$} \Comment{Right type (2) or right type (1) connection}
			\State \Return{$(d_1, 0, 0)$}
		\Else \Comment{The same for mini-tree level}
			\State $d_2 \gets d_1.in_2[n' - 1]$
			\State $n'' \gets below(d_1.in_2, n' - 1) + 1$
			\If{$d_2.size = 1$} \Comment{A single vertex mini-tree}
				\State \Return{$(d_1, d_2, 0)$}
			\ElsIf{$n'' \le d_2.ltype2_2$} \Comment{Left type (2) connections}
				\State \Return{$canonize(d_1, d_2, 0)$}
			\Else
				\State $n'' \gets n'' - d_2.ltype2_2$
				\If{$n'' > in\_size[d_2.size, d_2.rep, 0]$} \Comment{Right (2) or right (1)}
					\State \Return{$canonize(d_1, d_2, 0)$}
				\Else
					\State \Return{$canonize(d_1, d_2, in\_select[d_2.size, d_2.rep, n''])$}
				\EndIf
			\EndIf
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

Note that in the condition at ``Right (2) or right (1)'' the look-up table $in\_size$ comes from the implementation of the $in\_size$ operation and in this case it returns the number of in-order names assigned to all vertices of the micro-tree when it is considered independently.
In order to check the bounds of $in\_select$ before it is processed, we store a global field $in\_size\_0$ which contains the number of in-order names used in total.

\subsubsection{Leaf Operations}

We support the $leaf\_size(i)$ operation the same way we did support $subtree\_size$.
We support the operation $leaf\_first(i)$ which returns the first leaf in a subtree the same way we did with $deepest\_vertex$.
We than support $leaf\_rank(i)$ and $leaf\_select(n)$ the same way we did with $pre\_rank(i)$ and $pre\_select(n)$.

We can then restrict the leaf operations to a subtree:
\begin{algorithmic}
\Function{leaf\_rank}{$i, j$}
	\State \Return{$leaf\_rank(j) - leaf\_rank(leaf\_first(i)) + 1$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{leaf\_select}{$i, n$}
	\State \Return{$leaf\_select(n + leaf\_rank(leaf\_first(i)) - 1)$}
\EndFunction
\end{algorithmic}

\subsection{Ancestral Operations}

Let's assume a tree $T_1$ whose nodes are mini-tree roots.
A node $u$ is a parent of node $v$ if $v.parent_1 = u$.
This tree has at most $\frac{n}{B}$ nodes.
Similarly a $T_2$ tree for a mini-tree consists of nodes corresponding to micro-trees which are connected if $v.parent_2 = u$.

\subsubsection{LCA}

The operation $lca(i_1, i_2)$ is going to be solved on three levels: micro-tree, mini-tree and whole tree.
If $i_1$ and $j_2$ are within the same micro-tree we use a look-up table.
Otherwise we use a global or a mini-tree LCA structure which uses the technique from \ref{lemma:rmq2}.

\begin{lemma}
	We can solve the LCA operation on $T_1$ and $T_2$ using an index of additional $o(n)$ and $o(B)$ bits.
\end{lemma}
\begin{proof}
	Let's first assume a general tree $T$ with $n$ nodes.
	Assume that we have an array $E$ of nodes of the tree as we visit them during an Eulerian tour starting at the root.
	Then $LCA(u, v)$ is the shallowest node between the first occurrence of $u$ and the first occurrence of $v$ in the array $E$.
	We can therefore find the LCA using a $rmq$ query on an array $D[i] = depth(E[i])$, which we solve by precomputing a table of $\log n$ levels.
	This table requires $n \log^2 n$
	
	In each node of the tree, we store its depth and the index of its the first occurrence in the array $E$, which we do not store anywhere.
	The precomputed table contains directly names of the nodes and in the case of two overlapping intervals we compare the depths of the candidates.
	
	In case of $T_1$ and $T_2$, we use $depth_1$ and $depth_2$ as the relation of one node being ancestor of the other is preserved.
	The name of the node is its primary $\tau_1$ or $\tau_2$ name.
	The tree $T_1$ has $\frac{n}{B}$ nodes, so the space required is $O(\frac{n}{B} \log^2 n) = O(n \log^{2 - c})$, which forces the constant $c$ in definition of size $B$ to be $c \ge 3$ in order to keep the space $o(n)$.
	The tree $T_2$ has $\frac{B}{b}$ nodes, which results in space $O(\frac{B}{b} \log^2 B) = o(B)$.
\end{proof}

There are two interesting cases which need to be addressed in the algorithm.
If the vertices $i_1$ and $i_2$ are in a mini-tree or a micro-tree which share their root, the answer is the root.
If the vertices are in unaffiliated mini-trees or micro-trees, the reduction to LCA of their roots might not be correct if the answer was either of the roots.
We need to check if the path between them uses type (3) connection in a subtree of either of them, which we can do by another LCA query on the root of the type (3) connected mini-tree or micro-tree.
Note that in case of micro-trees, we are only interested in type (3) connection to a micro-tree within the same mini-tree.

\begin{algorithmic}
\Function{LCA}{$i_1, i_2$}
	\If{$i_1.\tau_1 = i_2.\tau_1$} \Comment{The same mini-tree}
		\If{$i_1.\tau_2 = i_2.\tau_2$} \Comment{The same micro-tree}
			\State \Return{$canonize(i_1.\tau_1, i_1.\tau_2, LCA[i_1.size, i_1.rep, i_1.\tau_3, i_2.\tau_3])$}
		\ElsIf{$i_1.primary_2 = i_2.primary_2$} \Comment{Type (1) connected micro-trees}
			\State \Return{$canonize(i_1.\tau_1, i_1.primary_2, 0)$}
		\Else \Comment{Unaffiliated micro-trees}
			\State $k \gets LCA_2(i_1.\tau_1, i_1.primary_2, i_2.primary_2)$
			\If{$k = i_1.primary_2 \booland i_1.dummy_2 \ne null \booland i_1.\tau_2 \ne i_1.dummy_1$}
				\State $k' \gets LCA_2(i_1.\tau_1, i_1.bottom_2, i_2.primary_2)$
				\If{$k \ne k'$} \Comment{Replace $i_2$ by the dummy vertex}
					\State \Return{$LCA(i_1, (i_1.\tau_1, i_1.\tau_2, i_1.dummy_2))$}
				\Else
					\State \Return{$canonize(i_1.\tau_1, k, 0)$}
				\EndIf
			\ElsIf{$k = i_2.primary_2 \booland i_2.dummy_2 \ne null \booland i_2.\tau_2 \ne i_2.dummy_1$}
				\State $k' \gets LCA_2(i_1.\tau_1, i_1.primary_2, i_2.bottom_2)$
				\If{$k \ne k'$} \Comment{Replace $i$ by the dummy vertex}
					\State \Return{$LCA(i_2, (i_2.\tau_1, i_2.\tau_2, i_2.dummy_2))$}
				\Else
					\State \Return{$canonize(i_1.\tau_1, k, 0)$}
				\EndIf
			\Else \Comment{The root must be the answer}
				\State \Return{$canonize(i_1.\tau_1, k, 0)$}
			\EndIf
		\EndIf
	\ElsIf{$i_1.primary_1 = i_2.primary_1$} \Comment{Type (1) connected micro-trees}
		\State \Return{$i_1.primary_1, 0, 0$}
	\Else \Comment{Unaffiliated mini-trees}
		\State $k \gets LCA_1(i_1.primary_1, i_2.primary_1)$
		\If{$k = i_1.primary_1 \booland i_1.dummy_1 \ne null$} \Comment{Test Connection type (3)}
			\State $k' \gets LCA_1(i_1.bottom_1, i_2.primary_1)$
			\If{$k \ne k'$} \Comment{Replace $i_2$ by the dummy vertex}
				\State \Return{$LCA(i_1, (i_1.\tau_1, i_1.dummy_1, i_1.dummy_1.dummy_2))$}
			\Else
				\State \Return{$(k, 0, 0)$}
			\EndIf
		\ElsIf{$k = i_2.primary_1 \booland i_2.dummy_1 \ne null$} \Comment{Symmetric}
			\State $k' \gets LCA_1(i_1.primary_1, i_2.bottom_1)$
			\If{$k \ne k'$} \Comment{Replace $i$ by the dummy vertex}
				\State \Return{$LCA(i_2, (i_2.\tau_1, i_2.dummy_1, i_2.dummy_1.dummy_2))$}
			\Else
				\State \Return{$(k, 0, 0)$}
			\EndIf
		\Else \Comment{The root must be the answer}
			\State \Return{$(k, 0, 0)$}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

\subsubsection{Level Ancestor}

We again use the trees $T_1$ and $T_2$, and solve the $level\_ancestor(i, d)$ operation for it.
We in fact need a more general operation \emph{weighted level ancestor}, which works on a graph with edges having assigned weight, which is a natural number.
We are looking for the first node $j$ such that the sum of weights on the path from $i$ to $j$ is maximum possible but less than $d$.
There is a structure $T\_{lrm}$ which can be used to solve this problem, and which we have used before to implement $fwd_search$ in FF structure \ref{Tlrm},

The nodes are assigned depths -- in case of $T_1$ it will be absolute depths $depth_1$, in case of $T_2$ it will be relative depths inside the mini-tree $depth_2$.
There will not be any artificial root, as $T_1$ and $T_2$ are trees with an existing root.
The root is assigned the depth equal to $0$.
We can use the algorithm $lrm\_search(n, v)$ with a minor modifications: we are looking for a node which is in the maximum depth less than $v$ instead of at least $v$.
This is necessary because we want to get to the root of a tree which is just below the correct tree; then we use parent function to get to the deepest ancestor in the correct tree.
This difference be addressed by changing the function $succ_1$ into $prev_1$.
We also add an identifier to distinguish which tree we are searching through.

We first identify the correct mini-tree by using $anc\_search$ on $T_1$, then we identify the correct micro-tree by using $anc\_search$ on $T_2$, and finish the operation with a look-up table in a micro-tree.

\begin{algorithmic}
\Function{level\_ancestor}{$i, d$}
	\State $v \gets depth(i) - d$
	\If{$v < 0$}
		\State \Return{$-1$}
	\EndIf
	\If{$v < depth(root_1(i))$} \Comment{Wrong mini-tree}
		\State $i \gets parent(anc\_search_1(root_1(i), v))$
	\EndIf
	\If{$v < depth(root_2(i))$} \Comment{Wrong micro-tree}
		\State $i \gets parent(anc\_search_2(i.\tau_1, root_2(i), v))$
	\EndIf
	\State \Return{$canonize(i.\tau_1, i.\tau_2, level\_ancestor[i.size, i.rep, i.\tau_3, depth(i) - v])$}
\EndFunction
\end{algorithmic}

\subsection{Level Operations}

\subsubsection{Level First and Level Last}

We first show how to support the operation $level\_first(i, d)$ restricted to a subtree of a vertex $i$.
The operation $leve\_last$ is analogous and we address the differences at the end.

The operation $level\_first$ is closely related to other select operations which we have already shown.
For each mini-tree root $i$ we have an array $lfirst_1$:
$$ i.lfirst_1[j] = \argmin_{\substack{pre\_rank(v) \ge pre\_rank(i) \\ depth(v) = depth(i) + j}} pre\_rank(v) $$
This array contains level-first descendants of $i$ for $j \le height(i)$ as it has the correct depth and it is the first one such.

We cannot afford to store the arrays $lfirst_1$, however we observe the following fact:
If for two mini-tree roots $i$, $j$ and level $d$, $i.lfirst_1[d - depth(i)] = j.lfirst_1[d - depth(j)]$, then $i.lfirst_1[d - depth(i) + k] = j.lfirst_1[d - depth(j) + k]$ for $k \ge 0$.

\begin{lemma}\label{l:level-first-3}
	There are at most three runs of a $\tau_1$ name in the array $lfirst_1$.
\end{lemma}
\begin{proof}
	Let's assume that a root $v$ of a mini-tree $t$ is a level-first descendant of $i$, then $v.\tau_1$ is in the array.
	There are two possibilities how a run of level-first descendants can be interrupted:
	\begin{enumerate}
		\item We look at the first child of $v$: if it is not in the same mini-tree, then $v$ must have some type (2) connections to the left $L$.
		If $h = \max_{l \in L} height(l)$ is greater than the height of $t$ (restricted to $t$), then $v.\tau_1$ will never occur in the array again.
		Otherwise, the level-first descendants of $v$ will occur starting with $d = depth(v) + h + 1$.
		\item If a level-first descendant of $v$ is a dummy vertex, then level-first descendants of its mini-tree will be in the array.
		If the height of the type (3) connected mini-tree is less than the height of the mini-tree $t$ (restricted to $t$), then level-first descendants of $v$ will again be in the array.
	\end{enumerate}
	These two causes of interruption can overlap, however they can cause at most three runs of $v.\tau_1$.
	
	If the root was not the level-first descendant, however another vertex of the mini-tree $t$ occurs in the array, we can apply the same reasoning about the second type of interruption.
	That proves that such $\tau_1$ can appear in at most two runs.
\end{proof}

We build a tree $T_1$ as a trie from the reversed arrays $lfirst_1$ with an additional artificial root.
We compress the tree by contracting paths between nodes which are either:
\begin{enumerate}
	\item the artificial root node;
	\item nodes corresponding to mini-tree roots.
	All leaves of $T_1$ are mini-tree roots, however not all mini-tree roots are leaves in $T_1$.
	\item branching nodes in the tree $T_1$.
	Their number is bounded by number of leaves of $T_1$.
	\item nodes corresponding to vertices in arrays $lfirst_1$ such that their predecessor has a different $\tau_1$ name.
\end{enumerate}
Not counting branching nodes, there are at most three vertices from each mini-tree which follows from the previous lemma.
The result is that the compressed tree contains $O(\frac{n}{B})$ nodes.

Let $h$ be the height of the original tree $h = height(0, 0, 0)$, then we associate each node $n$ with number $m = h - depth(n)$ where the depth in measured the original tree; the root is assigned $-\infty$.
We store the information about the nodes in an array; each node knows its $\tau_1$ name, its number $m$ and its pre-order number $k$ in $T_1$.
As the tree $T_1$ contains $O(\frac{n}{B})$ nodes, the structure requires $O(\frac{n}{B} \log^2 n)$ bits.
We also store the number $k$ corresponding to the mini-tree root as $lfirst\_k_1$ in the mini-tree structure.

We use the algorithm $lcm\_search$ which returns the first ancestor which has a greater or equal value than the one which we are searching for.
In our case, the function returns a $\tau_1$ name.
Note that in a general case, we should check whether the node which we found is a descendant of the queried node, which can be done by $is\_ancestor$.
However, in the algorithm as we formulate it, it is not necessary.

We can now reduce finding the correct mini-tree containing the level-first descendant to querying weighted ancestor in the tree $T_1$.

We build a similar structure for querying weighted level ancestor for each mini-tree, restricted to its vertices.
For that we also need to store $height_1$ of the mini-tree root, and we do the same for micro-tree roots.
We use it to find the correct micro-tree containing the level-first descendant.
Finally, we use a look-up table to find the answer within the correct micro-tree.

\begin{algorithmic}
\Function{level\_first}{$i, d$}
	\If{$d < depth(i) \booland d > depth(i) + height(i)$}
		\State \Return{$-1$} \Comment{Answer does not exist}
	\EndIf
	
	\State $C = \O$ \Comment{Set of candidates}
	\If{$i.\tau_2 = 0 \booland i.\tau_3 = 0$} \Comment{Find correct mini-tree}
		\State $h_1 \gets height(0, 0, 0)$
		\State $i \gets lcm\_search_1(i.lfirst\_k_1, h_1 - d)$
		\State $correct_1 \gets 1$ \Comment{We are sure about the mini-tree}
	\Else
		\State $correct_1 \gets 0$ 
	\EndIf
	
	\If{$i.\tau_3 = 0$} \Comment{Find correct micro-tree}
		\If{$i.dummy\_ancestor \booland correct_1 = 0$}
			\State $c \gets level\_first(dummy\_down(dummy_1(i)), d)$
			\If{$c \ne -1$} \Comment{Level-first in type (3) connected mini-tree}
				\State $C = C \cup \{c\}$
			\EndIf
		\EndIf
		\If{$d \le depth(root_1(i)) + height_1$}
			\State $i \gets lcm\_search_2(i.\tau_1, i.lfirst\_k_2, height_1 - (d - depth(root_1(i))))$
			\State $correct_2 \gets 1$ \Comment{We are sure about the micro-tree}
		\Else
			\State $correct_2 \gets 2$ \Comment{The solution is not in this mini-tree}
		\EndIf
	\Else
		\State $correct_2 \gets 0$
	\EndIf

	\If{$is\_ancestor[i.size, i.rep, i.\tau_3, i.dummy_2] \booland correct_1 = 0 \booland correct_2 = 0$}
		\State $c \gets level\_first(dummy\_down(dummy_2(i)), d)$
		\If{$c \ne -1$} \Comment{Level-first in type (3) connected mini-tree or micro-tree}
			\State $C = C \cup \{c\}$
		\EndIf
	\EndIf
	
	\If{$correct_2 \ne 2 \booland d \le depth(root_2(i)) + height_2$}
		\State $c \gets level\_first[i.size, i.rep, i.\tau_3, d - depth(i)]$
		\If{$c \ne -1 \booland c \ne i.dummy_2$} \Comment{Level-first in the micro-tree}
			\State $C = C \cup \{c\}$
		\EndIf
	\EndIf
	
	\State $first \gets -1$
	\ForAll{$c \gets C$} \Comment{Selecting the left-most candidate}
		\If{$first = -1 \boolor pre\_rank(c) < pre\_rank(first)$}
			\State $first \gets c$
		\EndIf
	\EndFor
	\Return{$first$}
\EndFunction
\end{algorithmic}

\paragraph{Level Last}

In case of level-last descendant operation, we define the arrays $llast_1$ as:
$$ i.llast_1[j] = \argmax_{\substack{pre\_rank(v) \le pre\_rank(i) + subtree\_size(i) - 1  \\ depth(v) = depth(i) + j}} pre\_rank(v) $$

The only change necessary in the algorithm is selecting the right-most candidate, which is the one with highest pre-order rank.

\subsubsection{Level Next and Level Previous}

We focus on $level\_next(i, j)$; the operation $level\_prev(i, j)$ is symmetrical.

We distinguish four cases:
\begin{itemize}
	\item the vertex $j$ is the level-last vertex in the subtree of $i$.
	Then there is no next vertex on the level.
	\item the vertex $j$ is not the level-last vertex in its micro-tree.
	We use a look-up table to handle this case.
	We also need to check for type (3) connected subtree which could contain the answer.
	This check can be easily done by comparing pre-order rank of $j$ and $j.dummy_2$.
	\item the vertex $j$ is the level-last vertex in its micro-tree, then we search the micro-tree $t_2$ which is ``to the right'' from $j$.
	We find its $\tau_2$ name using a structure which we show later.
	We distinguish two cases:
	\begin{itemize}
		\item $t_2$ contains a dummy vertex and $j$ is in its subtree, then we solve the search in $t_2$ using a look-up table searching for the first vertex $v$ on the desired level with $pre\_rank(v) > pre\_rank(dummy_2)$.
		\item otherwise, it is the level-first vertex in the micro-tree.
	\end{itemize}
	\item the vertex $j$ is the level-last vertex in its mini-tree.
	We need to find vertex in the mini-tree $t_1$ which is ``to the right'' from the current one.
	As before, we distinguish two cases:
	\begin{itemize}
		\item $j$ is a descendant of the dummy vertex in $t_1$, then we need to find the micro-tree which contains the vertex $v$ such that:
		$$ v = \argmin_{\substack{depth(k) = depth(j) \\ pre\_rank(root_1(t))> pre\_rank(k) \\ pre\_rank(k) > pre\_rank(dummy_1(t)) + subtree\_size(dummy_1(t))}} pre\_rank(k)$$
		As there is at most one dummy vertex in a mini-tree, we store a compressed array $dummy\_next$ which contains $\tau_2$ names of micro-trees which contain the answer for all admissible levels.
		This compressed array contains at most $B$ elements in $3b$ runs (which follows from lemma \ref{l:level-first-3}) and therefore used $O(b \log B) = o(B)$ bits of space.
		Let $t_2$ be the micro-tree from the array $dummy\_next$, then we finish the query as in the previous case.
		\item otherwise, the answer is the level-first vertex of the mini-tree $t_1$.
	\end{itemize}
\end{itemize}

We design a structure which we use to solve the second and the third case.
Let's assume a graph $G$ whose nodes are mini-trees and each two nodes $x, y$ are connected whenever there is a vertex $u$ in $x$ and a vertex $v$ in $y$ such that $level\_next(i, u) = v$.

\begin{lemma}\label{l:level-graph}
	The graph $G$ has $O(\frac{n}{B})$ edges.
\end{lemma}
\begin{proof}
	The graph is planar, therefore there is a linear bound on number of edges.
\end{proof}

For each mini-tree $i$ we create a compressed array $lnext_1$:
$$i.lnext_1[j] = level\_next((0, 0, 0), level\_last(i, depth(i) + j))$$
If there is no next vertex, than we use the name $-1$.

These compressed arrays contain at most $n$ elements in total, and according to the previous lemma, there are only $O(\frac{n}{B})$ runs in all compressed arrays.
The bound on number of runs follows from lemmas \ref{l:level-first-3} and \ref{l:level-graph}
The extra ``no next vertex'' names do not matter as we could have equivalently defined that $level\_next((0, 0, 0), level\_last((0, 0, 0), d)) = level\_first((0, 0, 0), d)$.
The collection of compressed arrays requires $O(\frac{n}{B} \log n) = o(n)$ bits of space.

We construct a similar structure $lnext_2$ for all micro-trees, storing the $\tau_2$ name of the right micro-trees.
This requires $O(\frac{B}{b} \log B) = o(B)$ bits of memory.

\begin{algorithmic}
\Function{level\_next}{$i, j$}
	\If{$j = level\_last(i, depth(j))$}
		\State \Return{$-1$}
	\EndIf
	
	\State $k \gets level\_next[j.size, j.rep, j.\tau_3]$ \Comment{Search in micro-tree}
	\If{$k = -1 \boolor j.dummy_2 \ne null \booland j.dummy_2 < k$} \Comment{Check dummy vertex}
		\State $f \gets level\_first(dummy\_down(dummy_2(j)), depth(j))$
		\If{$f \ne -1$}
			\State \Return{$f$}
		\EndIf
	\ElsIf{$k \ne -1$}
		\State \Return{$j.\tau_1, j.\tau_2, k$} \Comment{Answer in micro-tree}
	\EndIf
	
	\State $t_2 \gets j.lnext_2[depth(j) - depth(root_2(j))]$ \Comment{Search for micro-tree}
	\If{$t_2 \ne -1 \booland t_2.dummy_2 \ne null \booland is\_ancestor(dummy\_down(dummy_2(t_2)), j)$}
		\State \Return {$level\_next'[t_2.size, t_2.rep, depth(j) - depth(root_2(t_2))]$}
	\ElsIf{$t_2 \ne -1$}
		\State \Return{$level\_first(t_2, depth(j))$}
	\EndIf
	
	\State $t_1 \gets j.lnext_1[depth(j) - depth(root_2(j))]$ \Comment{Search for mini-tree}
	\If{$t_1 \ne -1 \booland t_1.dummy_1 \ne null \booland is\_ancestor(dummy\_down(dummy_1(t_1)), j)$}
		\State $t_2 \gets t_1.dummy\_next[depth(j) - depth(dummy_1(t_1))]$
		\If{$t_2.dummy_2 \ne null \booland is\_ancestor(dummy\_down(dummy_2(t_2)), j)$}
			\State \Return {$level\_next'[t_2.size, t_2.rep, depth(j) - depth(root_2(t_2))]$}
		\Else
			\State \Return{$level\_first(t_2, depth(j))$}
		\EndIf
	\ElsIf{$t_1 \ne -1$}
		\State \Return{$level\_first(t_1, depth(j))$}
	\Else
		\State \Return{$-1$}
	\EndIf
\EndFunction
\end{algorithmic}

\subsection{Final Thoughts}

In the beginning we defined a constant $c \ge 2$ which can be set to $2$ as long as we do not require the ancestral and level operations in the form we have shown (they require $c \ge 3$).
However, it is not true $c >2$ is required as \todo{ref} shown the same structure with slightly more complicated operations for which $c = 2$ is sufficient.
We chose the simpler options as they use the same structures which we used in other parts of this work.

The structure supports a very similar spectrum of operations as FF while being easier to extend as it is often sufficient to implement the operation on mini-tree and micro-tree scales using techniques inspired by traditional solutions, and adding a look-up table for queries contained within micro-trees.

\section{Universal Succinct Representation}

All the previous structures can be split into two parts (recalling the definition of systematic data structures):
\begin{enumerate}
	\item data -- in case of BP (and FF -- it shares the data part of the representation) and DFUDS it is clearly the bit string $S$ of size $2n$ bits.
	In case of TC it could be the structure stripped off all fields which are not necessary for reconstruction of the original tree.
	We can go even further and split the data into the sequence of pairs $(size, rep)$ for each micro-tree and a simple structure which maintains their relationships (number of mini-trees in the tree, number of micro-trees in a mini-tree, parents, primaries).
	From now on we will treat the simple structure as an index.
	\item index -- any structure or structures of size $o(n)$ bits which speeds up the queries; for BP and DFUDS they were rank, select, match, rmq indices.
	In case of TC we incorporated the index alongside the micro-tree $rep$s, however as the structure takes only $o(n)$ bits (the tables of offsets of mini-trees, and micro-trees), we can extract it and keep it in a parallel structure.
\end{enumerate}

Any algorithm for word-RAM can access only $w = \O(\log n)$ consecutive bits of memory in each step.
As all the operations which we have shown were formulated for word-RAM, we can replace the data bit-string itself with a structure which answers queries on $w$ consecutive bits in time $O(1)$.
As long as the structure uses only $2n + o(n)$ bits, the resulting data structure is still succinct.

We define a structure for a universal succinct representation which provides three operations running in time $O(1)$:
\begin{itemize}
	\item $BP\_substring(i, b)$ -- returns $b$ bits of BP representation staring at position $i$.
	\item $DFUDS\_substring(i, b)$ -- returns $b$ bits of DFUDS representation staring at position $i$;
	\item $TC\_microtree(\tau_1, \tau_2)$ -- returns the micro tree ($\tau_1, \tau_2$).
	We assume that the decomposition and the naming schema which we described in the section \todo{ref TC} is used.
\end{itemize}

The parameter $b$ can be in range $[1, w]$.
However, from now on we fix it to $b = \frac{\log n}{16}$ and call it a block size.
The answer to any other value of the parameter can be obtained by $O(1)$ queries with the fixed $b$ with an optional trim of unwanted bits.

Assuming that a structure with such properties exist, we can immediately build a succinct structure which supports the union of all operation described in sections BP, DFUDS, FF and TC by adding the right indices of size $o(n)$.
The advantage is also that any progress in any of the mentioned structures becomes immediately available in this structure by simply adding the necessary index.

\subsection{Restriction to Mini-Trees}

We start with the same two-level decomposition as in the Tree Covering representation.
First, we focus on supporting BP and DFUDS operations, which we do on a mini-tree level.

We define two compressed arrays $BP$ and $DFUDS$: for each bit (except for the first one in case of $DFUDS$) we store $\tau_1$ name of the mini-tree to which the vertex whose bit it is belongs.
Contrary to \todo{ref article} we associate the bits in unary degree sequence of vertex $v$ with the vertex $v$ and not his individual children.
This simplifies the data structure.

\begin{lemma}\label{l:usr-runs}
	Every $\tau_1$ name occurs in at most 4 runs in the compressed array $BP$ and 3 runs in $DFUDS$.
\end{lemma}
\begin{proof}
	Because the definitions of the BP and DFUDS representations are based on pre-order traversal of the tree, and they are recursive, we know all vertices between the first and last mentions of $\tau_1$ are in the subtree of the mini-tree root $v$.
	
	We can assume that the root $v$ of a mini-tree $t$ belong to $t$, otherwise the BP representation lacks the first and last mention of $t.\tau_1$, which can only decrease the number of runs.
	In case of DFUDS, the root representation is continuous leading to omission of $degree(root_1(t))$ first occurrences of $t.\tau_1$.
	
	We also assume that the mini-tree $t$ contains at least two vertices, otherwise it is trivial.
	We call the own the children of $v$ which belong to the mini-tree $v$.
	
	There are three types of connections which connect other mini-trees to $t$.
	All type (1) and type (2) connections involve $v$ -- some subtrees rooted in children of $v$ can belong to other mini-trees.
	The own children of $v$ form an interval which is not interrupted by any type (1) nor type (2) connections.
	In BP representation the type (1) and type (2) connected mini-trees are either just after the opening parenthesis of $v$ or just before the closing parenthesis of $v$.
	They can separate the opening and closing parentheses from the consecutive block of children in $t$.
	In case of the DFUDS representation, the type (1) and type (2) connections can split the representation of $v$ from its children in $t$.
	However, in comparison to BP, the last occurrence of $t.\tau_1$ is in the subtree of the last own child of $v$, which precedes representation of any connections to the right from children in $t$.
	
	Each mini-tree can have up to one type (3) connection which can break the run of own children of $v$ in two.
\end{proof}

By the lemma, there are at most $O(1)$ runs, therefore the size of $BP$ and $DFUDS$ is $o(n)$.

We also immediately solve all queries which are not fully contained within a single mini-tree and all queries which involve any bit of root's representation.
For each run, we store the position where it starts and where it ends together with the preceding, first, last and following $w$ bits of the BP and DFUDS representations.
In case of DFUDS we replace the first $w$ bits of the first run (where the root start) by $w$ bits starting at the position of root's closing parenthesis.
This requires $O(\log n) + w = o(B)$ bits per mini-tree.

The queries of this type can be detected by $O(1)$ tests.
The only case which is more complicated is query on bits from DFUDS representation of the mini-tree root $v$.
There we need to generate the right number of opening parentheses which are part of the degree sequence of $v$.
All this can be implemented using arithmetic and bitwise operations on word-RAM.

From now on, we can assume that all queries can be reduced to only a single mini-tree.
We also don't have to handle specially the case of existence mini-tree dummy vertex, because any query in which it could be involved is not contained in the mini-tree and therefore it was already solved.
It is still necessary to keep it in order to calculate the degree of its parent correctly for DFUDS representation.
The position $i$ gets transformed so that it is a position inside the mini-tree including the mini-tree root; we do this by the function $before$ applied to $BP$ or $DFUDS$ compressed arrays and corrections due non-own children of potentially shared mini-tree root.

$$
i' = \begin{cases}
	before(BP, i) & \textrm{if BP and}\ BP[i].\tau_1 = BP[i].primary_1 \\
	before(BP, i) + 1 & \textrm{if BP, otherwise} \\
	before(DFUDS, i) - DFUDS[i].degree + DFUDS[i].degree\_own & \textrm{if DFUDS and}\ DFUDS[i].\tau_1 = DFUDS[i].primary_1 \\
	before(DFUDS, i) + DFUDS[i].degree\_own + 1 & \textrm{if DFUDS, otherwise}
\end{cases}
$$

\bigskip

We call a vertex \emph{significant} if its subtree size is larger than $\frac{\log n}{16}$.
A \emph{skeleton} of a mini-tree is the subtree induced by significant vertices, which by is connected as each ancestor is also significant.

There are at most $O(\frac{B}{\log n})$ leaves in the skeleton as the subtree of each of them contains at least $\frac{\log n}{16}$ vertices.
We can assume that there is also at least one significant vertex as otherwise the size of a mini-tree is less than $w$, and so every query is already has already been solved.

\subsection{Skinny Mini-Trees}

We call a mini-tree \emph{skinny} if its skeleton is a path.
We solve this special case first, and then generalize it to all skeletons.

We use the same notation as \todo{ref article}:
Let $P$ by the skeleton, which is a path, $u$ its leaf and $v$ the last child of $u$.
\begin{align*}
	S &= \{ s : parent(s) \in P \booland s \notin P \} \\
	S_D &= \{ s \in S : pre\_rank(s) \le pre\_rank(v) \} \\
	S_U &= \{ s \in S : pre\_rank(s) > pre\_rank(v) \}
\end{align*}

We represent a skinny mini-tree using four bit strings:
\begin{itemize}
	\item Path down, $P_D$ -- the concatenation of the unary degree sequences of vertices in the skeleton, assuming only their children in $S_D$.
	They are stored in the order from the root to $u$.
	\item Path up, $P_U$ -- the same for children in $S_U$ and direction from $u$ to the root.
	\item Trees on path down, $T_D$ -- subtrees of $S_D$ stored consecutively according to their pre-order numbers.
	We call the trees \emph{left dangling trees}.
	The trees can be stored in any self-delimiting representation which requires at most $2k - 1$ bits for a tree with $k$ vertices.
	We can either use BP without the first parenthesis, which is an opening one, or DFUDS without the artificially prepended opening parenthesis.
	\item Trees on path up, $T_U$ -- the same for subtrees of vertices $S_U$ ordered by their pre-order numbers.
	We call them \emph{right dangling trees}.
\end{itemize}

These four bit strings together use exactly $2$ bits per every vertex of the mini-tree.
Each opening parenthesis in $P_D$ and $P_U$ can be paired with the one missing in representations of the dangling trees in $T_D$ and $T_U$.
Vertices belonging to the path are represented by closing parentheses in $P_D$ and $P_U$, one in each.

We can restore the original mini-tree from this representation:
\begin{algorithmic}
\Function{restore}{}
	\State $v \gets null$
	\While{$P_D$ has more symbols}
		\State $v \gets$ new child of $v$
		\While{The symbol in $P_D$ is an opening parenthesis}
			\State Read a tree from $T_D$, decode it and add it as a child to $v$
		\EndWhile
	\EndWhile
	\While{$P_U$ has more symbols}
		\While{The symbol in $P_U$ is an opening parenthesis}
			\State Read a tree from $T_U$, decode it and add it as a child to $v$
		\EndWhile
		\State $v \gets parent(v)$
	\EndWhile
\EndFunction
\end{algorithmic}

\subsubsection{Generation of BP and DFUDS Representations}

\begin{lemma}
	Using an index of size $o(B)$, we can query $BP\_substring(j b, b)$ of any aligned block in constant time.
\end{lemma}
\begin{proof}
	We first formulate an algorithm which outputs the BP representation of the whole mini-tree.
	The algorithm is based on $restore$:
	
	\begin{algorithmic}
	\Function{BP\_rep\_skinny}{}
		\State $p_D \gets 0; p_U \gets 0; t_D \gets 0; t_U \gets 0$
		\While{$p_D < |P_D|$}
			\State $write("(")$
			\While{$P_D[p_D] = "("$}
				\State $t \gets read\_tree(t_D); t_D \gets t_D + |t|$
				\State $write(BP\_rep[t])$
				\State $p_D \gets p_D + 1$
			\EndWhile
			\State $p_D \gets p_D + 1$ \Comment{Consume the closing parenthesis}
		\EndWhile
		\While{$p_U < |P_U|$}
			\While{$P_U[p_U] = "("$}
				\State $t \gets read\_tree(t_U); t_U \gets t_U + |t|$
				\State $write(BP\_rep[t])$
				\State $p_U \gets p_U + 1$
			\EndWhile
			\State $p_U \gets p_U + 1$ \Comment{Consume the closing parenthesis}
			\State $write(")")$
		\EndWhile
	\EndFunction
	\end{algorithmic}
	
	For each $j$ we store the state of the algorithm: the instruction pointer and value of all local variables, which is $O(\log \log n)$ bits.
	Using this index we can restart the course of the algorithm from any position which related to the output is either a parenthesis of a vertex in $P$ or beginning of a dangling tree.
	In order to support any position of the output, we add to the index an offset $o$ which allows us to skip the first $o$ bits of a BP representation of a dangling tree.
	
	In order to generate any $b$ bits of BP representation, at most $b$ bits are read from $P_D$ or $P_U$ as well as up to $b + 2 \frac{\log n}{8}$ bits from $T_D$ or $T_U$.
	We can therefore implement the algorithm solely as a look-up table which is provided with all the bits which it may use; that is $4b + 4 \frac{\log n}{8} + 7 \log \log n + c < \log n$: the $\log \log n$ is for the offset $o$, values of $p_D, p_U, t_D, t_U$ and $|P_D|, |P_U|$, and $c$ is a constant for the instruction pointer.
	The size of the look-up table is $o(n)$ bits.
\end{proof}

\begin{lemma}
	Using an index of size $o(B)$, we can query $DFUDS\_substring(j b, b)$ of any aligned block in constant time.
\end{lemma}
\begin{proof}
	We use the same technique as in the previous lemma.
	The algorithm is a little more complicated.
	We need to to go through $P_D$ twice: the first time to report the degree, and the second time when we output the left dangling trees.
	We also go through $P_U$ twice: the first time from the end to the beginning treating closing parentheses as delimiters, and the second time to output the right dangling trees.
	
	There are two more local variable $p'_D$ and $p'_U$ for which we also need to provide $b$ bits of $P_D$ and $P_U$.
	In case of $p'_U$ we provide $b$ bits before the offset as the algorithm only decrements it.
	The key key of the look-up table has size $6b + 4 \frac{\log n}{8} + 9 \log \log n + c < \log n$.
	
	\begin{algorithmic}
	\Function{DFUDS\_rep\_skinny}{}
		\State $p_D \gets 0; p_U \gets 0; p'_U \gets |P_U| - 2; t_D \gets 0; t_U \gets 0$
		\State $write("(")$ \Comment{The artificial first parenthesis}
		\While{$p_D < |P_D|$}
			\State $p'_D \gets p_D$ \Comment{First we output the degree sequence}
			\While{$P_D[p'_D] = "("$} \Comment{Left dangling children}
				\State $write("(")$
				\State $p'_D \gets p'_D + 1$
			\EndWhile
			\State $p'_D \gets p'_D + 1$ \Comment{Consume the closing parenthesis}
			\If{$p'_D < |P_D|$} \Comment{Child in the path}
				\State $write("(")$
			\EndIf
			\While{$P_U[p'_U] = "(" \booland p'_U \ge 0$} \Comment{Right dangling children}
				\State $write("(")$
				\State $p'_U \gets p'_U - 1$
			\EndWhile
			\State $p'_U \gets p'_U - 1$ \Comment{Consume the closing parenthesis}
			\State $write(")")$ \Comment{End of unary degree sequence}
			\While{$P_D[p_D] = "("$} \Comment{Then we output the left dangling children}
				\State $t \gets read\_tree(t_D); t_D \gets t_D + |t|$
				\State $write(DFUDS\_rep[t])$
				\State $p_D \gets p_D + 1$
			\EndWhile
			\State $p_D \gets p_D + 1$ \Comment{Consume the closing parenthesis}
		\EndWhile
		\While{$p_U < |P_U|$} \Comment{Output the right dangling trees}
			\If{$P_U[p_U] = "("$}
				\State $t \gets read\_tree(t_U); t_U \gets t_U + |t|$
				\State $write(DFUDS\_rep[t])$
			\EndIf
			\State $p_U \gets p_U + 1$ \Comment{Consume the closing parenthesis}
		\EndWhile
	\EndFunction
	\end{algorithmic}
\end{proof}

It is possible to split the algorithms into several parts which touch only some bit strings, not all 4 of them at 6 different places.
This change would allow to generate blocks of a double size.

\subsection{General Mini-Trees}

If the skeleton is a general tree, we decompose it iteratively into paths.
We first remove the left-most path connecting the root with the first skeleton leaf; we call it left-leaning path.
For each tree in the rest of the skeleton, we remove its right-most path connecting the root with the last skeleton leaf; we call this path right-leaning.
We repeat this until every vertex of the skeleton is in a left-leaning or right-leaning path.

All subtrees which do not contain any significant vertex are connected to a significant vertex; they are again called left or right dangling trees.
The paths together with dangling trees form skinny mini-trees.
There are $O(\frac{B}{\log n})$ of them as the subtree size of each skeleton leaf is at least $\frac{\log n}{16}$.
We assign each path a number starting with $0$.

The paths are connected with each other:
\begin{itemize}
	\item left-leaning path can have some left-leaning and righ-leaning paths connected to the right.
	If there are more connections to one vertex, we store only the left-most connection.
	\item right-leaning path can have some left-leaning paths connected to the left.
	Again, if there are more of them connected to the same vertex, we store only the left-most one.
	\item all paths except for one have a next path.
	The next path is either the path containing the right sibling of its root if it exists, or the path containing the parent of its root.
	The only path which does not have next path is the path containing the mini-tree root.
\end{itemize}

In the skinny mini-trees we could uniquely reference any bit of the representation by the state $s$ of the algorithm and the offset $o$.
In case of general mini-trees, we add the number of the path.
The size of the triplet is $O(\log \log n)$ and we call it a \emph{reference}.
The index for all $j = i \% b$ is a reference.

\subsubsection{Path Structures}

We first add opening parentheses to the bit strings $P_D$ and $P_U$ for all connected paths.
This change adds $O(\frac{B}{\log n})$ bits because the each root of a path (except for the path containing the mini-root root) is represented by $3$ bits in total.

The structure for a path contains the following fields:
\begin{itemize}
	\item $P_D, P_U, T_D, T_U$ -- as before;
	\item $C_D, C_U$ -- compressed arrays of references to the nearest connection.
	We derive $C_D$ incrementally from $P_D$ and connections to other paths; similar derivation is used for $C_U$.
	\begin{align*}
		C'_D[i] &= 1 \iff P_D[i] = "(" \booland P_D[i]\ \textrm{is a root of a different path} \\
		C''_D[i] &= \textrm{reference to the root of the path at}\ succ(C'_D, i) \\
		C_D &= compressed\_array(C''_D)
	\end{align*}
	The compressed arrays $C_D$ and $C_U$ are stored in two version: for BP and DFUDS representations; the structures can be partly shared as only the $names$ arrays differ.
	Note that only one of $C_D$, $C_U$ need to be stored depending on whether the path is left-leaning or right-leaning.
	\item $next\_BP$, $next\_DFUDS$ -- references to the next path for both BP and DFUDS representations.
\end{itemize}

Each compressed array contains each run only once.
All references in the compressed arrays point to path roots, and every path root is referenced from only once.
Therefore, there are $O(\frac{B}{\log n})$ runs in total, and the space of all arrays together is $o(B)$ bits.

While the algorithms generate the BP or DFUDS representations, it can happen that they need to switch to a different path.
We extend the algorithms (look-up tables in fact) $BP\_rep\_skinny$ and $DFUDS\_rep\_skinny$ to support the connections and control the number of generated bits.
We provide them with two extra arguments, which are offsets:
\begin{itemize}
	\item $c_D = upper(C_D, p_D)$ -- the nearest connection on the way down;
	\item $c_U = upper(C_U, p_U)$ -- the nearest connection on the way up.
\end{itemize}
The algorithm returns as soon as it is about to output the dangling trees for $p_d = c_d$ or $p_u = c_u$, as they represent connections to different paths.

The algorithms also return more information:
\begin{itemize}
	\item the number of bits they generated;
	\item the generated bits;
	\item the situation which lead to the end:
	\begin{enumerate}
		\item sufficient number of bits was generated;
		\item a switch is necessary to at offset $c_d$;
		\item a switch is necessary to at offset $c_u$;
		\item a switch is necessary to $next$.
	\end{enumerate}
\end{itemize}

The algorithms returning a block $j$ of a BP and DFUDS representation of a mini-tree are the same:

\begin{algorithmic}
\Function{substring}{j, rep}
	\State $(path, state=(p_d, p_u, \ldots), offset) \gets index_{rep}[j]$
	\State $block \gets ""; bits \gets 0$
	\While{$bits < b$}
		\State $c_D \gets upper(paths[path].C_{D, rep}, p_D); c_U \gets upper(paths[path].C_{U, rep}, p_U)$
		\State $(bl, bi, end) \gets skinny_{rep}[\textrm{parts of bit strings}, state, c_D, c_U]$
		\State $block = concat(block, bl[offset, bi]); bits \gets bits + bi$
		\If{$end = 1$}
			\State $(path, state=(p_d, p_u, \ldots), offset) \gets paths[path].C_{D, rep}[p_D]$
		\ElsIf{$end = 2$}
			\State $(path, state=(p_d, p_u, \ldots), offset) \gets paths[path].C_{U, rep}[p_U]$
		\ElsIf{$end = 3$}
			\If{$paths[path].next_{rep} = null$}
				\State \Break
			\Else
				\State $(path, state=(p_d, p_u, \ldots), offset) \gets paths[path].next_{rep}$
			\EndIf
		\EndIf
	\EndWhile
	\State \Return{$block, bits$}
\EndFunction
\end{algorithmic}

It only remains to show that the number of iterations of the while loop is constant.
We inspect the graph of transitions between paths.

\todo{image}

First we observe that the subtree size of the skeleton leaf is at least $\frac{\log n}{16}$, and so the encoding of skeleton leaf produces at least $b$ bits.
No matter on which type of path and the direction, we always process a skeleton leaf after at most $5$ iterations of the while loop.

\subsection{Micro-Tree Representation}

We decompose the mini-tree $M$ to micro-trees as it is usual for Tree Covering representation.
We reuse lemma \ref{l:usr-runs} which in this case claims that a BP representation of a micro-tree forms at most four runs in the BP representation of a mini-tree.
We prefer the BP representation over DFUDS as we do not need to adjust the degree of the micro-tree root and also incorporating the micro-tree dummy vertex is easier.

The mini-tree-local offsets of the beginning and the end are stored for each run.
The length of each run is bounded by the size of a micro-tree, therefore $O(1)$ queries of the BP representation are necessary.
The size of BP representation of a micro-tree can be calculated from the stored offsets of the runs.
Since it is at most $\frac{\log n}{2}$ bits, we can use a look-up table to transcode it into any other representation which is native for the TC structure.

\section{Transformation to a Binary Tree}

\todo{Whole section}
