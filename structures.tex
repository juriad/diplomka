\chapter{Alternative Structures}

In the previous chapter we showed how rank and select operation (and later match and enclose) could be used on top of thee different encoding of tree into a bit string.
Here we present data structures, which are fundamentally different in either the primitive operations they use or that they are not-systematic -- they are not based on a single bit string with additional indices.

\begin{enumerate}
	\item Fully-Functional -- a data structure built on top of a sequence of matching parentheses, however it uses different primitives then rank and select;
	\item Tree Covering -- recursive decomposition of the tree into smaller trees which are then encoded;
	\item Unified Approach -- a structure which provides access to both BP and DFUDS representations at the same time.
\end{enumerate}

There are three direction where the development of succinct data structures for trees continued.
Later in time but close to what we have described is so called fully functional approach which keeps bit string of BP, however it replaces all indices for the primitive operations with a single index.
Rank and select are only implications of more general \emph{linear search operations}; ancestral and child operations are translated to range minimum queries.

Leaving the realm of pure bit strings, we can decompose the tree into smaller trees; we do this recursively.
We represent the trees on different levels in different ways.
There has been many decompositions proposed, trying to maintain the number of smaller trees limited and their sizes balanced.

Finally, there has been an attempt to diminish the differences between several representations -- a succinct structure which provides a view to a BP and DFUDS bit string representation.
Its advantage is it automatically benefits from any new index for either BP or DFUDS representation.

\section{FF}

All the previous data structures were based on operations rank and select, which do not take into account the structure of data which we are representing.
The operations match and enclose work only on proximity of the current vertex.

The first truly powerful operation was $lca$ -- it allows to ask queries which are not restricted to a neighborhood of any vertex.
This was achieved by using a range minimum query; it was also the only place where it proved to be useful.
What if we generalized $rmq$ and replaced our old set of operations thereby?
We did not show the algorithms for $level\_ancestor$ in BP and DFUDS for a reasons: they were too complicated, whereas here in the Fully Functional data structure, it is going to be one of the easiest operations.

\todo{intro}

\subsection{New Operations}

We follow on \ref{ss:rmq-def} and define operations on the array $S$ with function $g$.
\begin{itemize}
	\item $sum(S, g, i, j) = \sum_{k=i}^j g(S[k])$ -- it is equivalent to $G[j] - G[i - 1]$ from \ref{ss:rmq-def}.
	Since $rank$ operation will no longer be a primitive operation, we cannot use it to compute $G[i]$.

	\listpart{Linear search operations:}
	\item $fwd\_search(S, g, i, d) = \min_{j > i} \{j : sum(S, g, i, j) = d\}$ returns the first position such that the difference of values in $G$ is $d$;
	\item $bwd\_search(S, g, i, d) = \max_{j < i} \{j : sum(S, g, j, i) = d\}$ works the same way as $fwd\_search$ but searches in direction towards the beginning of the array.
	
	\listpart{We extend and generalize the range queries:}
	\item $rmq(S, g, i, j) = sum(S, g, 0, i - 1) + \min_{i \le k \le j} sum(S, g, i, k)$ -- returns the value of the minimum in the given range.
	\item $rmq\_size(S, g, i, j) = | \{ i \le k \le j : sum(S, g, i, k) = rmq(S, g, i, j) \} | $ -- returns how many times the minimum occurred in the given range;
	\item $rmq\_rank(S, g, i, j, k)$ -- returns how many minima there are in the given range up to position $k$.
	This can be directly implemented using $rmq$ and $rmq\_size$:
\begin{algorithmic}
	\Function{rmq\_rank}{$S, g, i, j, k$}
		\If{$rmq(S, g, i, j) \ne rmq(S, g, i, k)$}
			\State \Return{$0$}
		\Else
			\State \Return{$rmq\_size(S, g, i, k)$}
		\EndIf
	\EndFunction
\end{algorithmic}
	\item $rmq\_select(S, g, i, j, n) = \min_{k \ge i} rmq\_rank(S, g, i, j, k) = n$ -- returns position of $n$-th minimum in range $i, j$;
	\item $RMQ, RMQ\_size, RMQ\_rank, RMQ\_select$ are defined similarly.
\end{itemize}

We show how it is possible to realize all operations which we used as primitives in the previous succinct data structures only by searches and range queries on calculated arrays.
We immediately get a data structure for BP and DFUDS representations which supports all queries which were supported before.
Later we show that even more operations which were impossible or required complicated indices are now feasible.

Ranking and selecting:
\begin{align*}
	rank_1(i) &= sum(S, \phi, 0, i) \\
	select_1(n) &= fwd\_search(S, \phi, 0, n) \\
	rank_0(i) &= sum(S, \psi, 0, i) \\
	select_0(n) &= fwd\_search(S, \psi, 0, n)
\end{align*}

Operations on sequences of matching parentheses:
\begin{align*}
	find\_close(i) &= fwd\_search(S, \pi, i, 0) \\
	find\_open(i) &= bwd\_search(S, \pi, i, 0) \\
	enclose(i) &= bwd\_search(S, \pi, i, 2) \\
	enclose(i_1, i_2)&\ \textrm{stays the same} \\
	rmqi(E, i, j) &= rmq\_select(S, \pi, i, j, 1) \\
	RMQi(E, i, j) &= RMQ\_select(S, \pi, i, j, 1)
\end{align*}

\subsection{The Data Structure}

As we are describing a universal structure independent on its specific usage for representing trees, we again use $N$ to denote the size of the bit string.
We assume a general $\pm 1$-function $g$.

The bit string $S$ is first split into blocks, which are roughly of polylogarithmic size.
Each block is then split into small blocks of size $\frac{\log N}{2}$.
Queries withing a small block are processed by look-up tables.
If the query spans multiple small blocks withing a single block, the queries are handled by min-max trees.
The queries spanning multiple blocks are answered by a macro structure.

\subsubsection{Small Blocks}

The lowest level of small blocks of size $b = \frac{\log N}{2}$ is the easiest.
The small blocks are small enough to by used as indices of precomputed look-up tables, which answer all queries in constant time.
We require the following look-up tables:
\begin{itemize}
	\item For given range $i, j$ returns the sum $v$.
	\item For given position $i$ and value $d$, returns the first next and previous position $p$ where the difference is $d$.
	\item For given range $i, j$ returns the local value $v$ of minimum and maximum.
	\item For given range $i, j$ returns the number $n$ of occurrences of minima and maxima.
	\item For given range $i, j$ and $n$, return the position $p$ of $n$-th minimum and maximum in the small block.
\end{itemize}
The values of $i, j, n$ are in range $[0, b - 1]$; the values $d, v$ are in range $[-b, b]$; the value of $p$ is in range $[0, b - 1]$ plus one special value indicating that such position does not exist in the small block.
Together we have 8 precomputed tables; each of them requires $O(\sqrt{N} \log N \log \log^3 N)$ bits of memory.

The tables are used to answer all range operations whenever $i$ and $j$ are in the same small block.
In case of searches, they also determine that the answer lay outside of the block.

\subsubsection{Min-Max Tree}

Each block contains $k^c$ small blocks for $k = \frac{\log N}{\log \log N}$ and an arbitrary constant $c \le 1$.
The $l$-th block covers an interval of $B = b k^c$ bits spanning from position $(l-1) B$ to $l B - 1$.
To simplify the description we isolate the block by shifting the offsets so that it covers the range $[0, B - 1]$.
We also assume that the values in the array $G$ are in range $[-B, B]$; the global values can be computed simply by adding the value $G[(l-1) B - 1]$.
This does not have any impact on correctness of the operations on the block level.

We build a perfect $k$-ary tree on top of the sequence of small blocks; we call it a \emph{min-max tree}.
The tree has a constant height equal to $c$.
Its leaves represent the information from small blocks provided by look-up tables; the inner nodes aggregate information from their children.
We store the following values in each node spanning the range $i, j$:
\begin{itemize}
	\item $e = sum(S, g, 0, j)$ -- the value at the end of the range;
	\item $m = rmq(S, g, i, j), ms = rmq\_size(S, g, i, j)$ -- the value of minimum and how many times it occurred;
	\item $M = RMQ(S, g, i, j), Ms = RMQ\_size(S, g, i, j)$ -- the value of maximum and how many times it occurred.
\end{itemize}

The aggregation in inner node is obvious: the last value for $e$; minimum for $m$, maximum for $M$; sum for $ms$ and $Ms$.
All values within a node require only $O(\log b)$ bits each.

We number the nodes of the tree in a heap-like fashion and store their values in arrays $e[\cdot], m[\cdot], ms[\cdot], M[\cdot], Ms[\cdot]$.
Note that all children of a node are stored together in consecutive $O(k \log \log N) = O(\log N) = c' b$ bits of memory, for a constant $c'$.
There are $\frac{k^c}{k-1} = O(k^{c-1})$ nodes, each of them requires $O(\log b)$ bits.
The density of the tree built on top of the block is $\frac{O(k^c \log b)}{k^c b} = O(\frac{\log b}{b}) = o(1)$.

\bigskip

We show how we support the operations on block level using traversal of the min-max tree and possibly a delegation to small blocks.
By $b(i) = \lfloor\frac{i}{b}\rfloor$ we denote the index of the small block which contains $i$.
To simplify the notation we assume that the node numbers of leaves are the same as indices of the small blocks with which they are associated.

\begin{algorithmic}
\Function{sum\_block}{$S, g, i, j$}
	\State $x \gets e[b(i - 1) - 1] + sum\_small\_block(S_{b(i - 1)}, g, 0, (i - 1) \% b)$
	\State $y \gets e[b(j) - 1] + sum\_small\_block(S_{b(j)}, g, 0, j \% b)$
	\State \Return{$y - x$}
\EndFunction
\end{algorithmic}

\paragraph{Search Operations}

\begin{lemma}\label{l:search}
	The result of search operations starting at $i$ and looking for a difference $d$ (which does not have a result in the small block $h$ containing $i$) lays in the first small block $j$ such that $m[j] \le v = sum(S, g, 0, ) + d \le M[j]$.
	We say that the value $v$ is covered by the extremes (minimum and maximum).
	
	No small block in subtree of a node of the min-max tree whose extremes do not cover the value $v$ contains the answer.
\end{lemma}
\begin{proof}
	The function $g$ which is used for calculating the array $G$ requires that $| G[k] - G[k-1] | \le 1$, which is the property introduced in \ref{ss:rmq-def}.
	From that follows that each small block contains all values between the minimum and maximum in the array $G$.
	The first small block which satisfies the condition on the extremes contains the desired value $v$.
	
	Moreover, if $v < \textrm{minimum of the rest of the block } b(i)$, then it is sufficient to find the first small block $j$ such that $m[j] \ge v$ because $M[j] \ge m[j-1]$.
	Similar statement holds for $v$ greater than the maximum.
	
	A leaf is associated with a small block and so its extremes are the same as of the small block.
	By contradiction, if a small block in a subtree contained the answer, the extremes of the leaf would cover $v$.
	An internal node aggregates the minimum and maximum of its subtree and so if its child contained the answer, the aggregation would assure that the extremes of the node cover $v$.
\end{proof}

When we process the search operations, we first check the containing small block for an answer.
If the answer is present there, we return the position and end.

Otherwise, we compute the desired value $v$ and traverse the tree up until we find the answer or until we reach the root.
If we get to root, we signal that this block does not contain the answer by returning a special value.
When we are in node $a$, we check the extremes of its siblings in the direction in which we are searching.
This check can be performed in time $c'$ using precomputed look-up tables as all siblings are stored in consecutive $c' b$ bits.

When we find such sibling, we move to it and start descending to the first child whose extremes cover the value $v$.
Finding such child again requires time $c'$ and another set of precomputed tables.
Once we descend into a leaf $j$, we compute the value $d' = v - e[j - 1]$ which we are going to look for in the associated small block using a look-up table. 
As the height of the tree is $c$, the whole operation takes at most $O(2 c c') = O(1)$ steps.

By an asterisk we denote a repeated application of look-up table on a list of consecutive nodes in the tree.

\begin{algorithmic}
\Function{fwd\_search\_block}{$S, g, i, d$}
	\State $p \gets fwd\_search\_small\_block(S_{b(i)}, g, i \% b, d)$
	\If{$p \ne -1$}
		\State \Return{$p + b(i) b$}
	\Else
		\State $v \gets e[b(i) - 1] + sum\_small\_block(S_{b(i)}, g, 0, i \% b) + d$
		\State $n \gets b(i)$ \Comment{The current node, initialized to be a leaf}
		\While{$(j \gets table_1^*[right\_siblings(n), v]) = -1$} \Comment{Sibling covering $v$}
			\State $n \gets parent(n)$
			\If{$is\_root(n)$}
				\State \Return{$-1$}
			\EndIf
		\EndWhile
		\While{$\boolnot is\_leaf(j)$}
			\State{$j \gets table_1^*[children(j), v]$} \Comment{The first child covering $v$}
		\EndWhile
		\State $d' \gets v - e[j - 1]$ \Comment{The remaining difference}
		\State \Return{$j b + fwd\_search\_small\_block'(S_{j}, g, 0, d')$} \Comment{$d'$ in block $j$}
	\EndIf
\EndFunction
\end{algorithmic}

Note that the final $fwd\_search\_small\_block'$ is the standard $fwd\_search\_small\_block$ altered to allow the answer $0$, which is prohibited by the original definition.
This can be handled with a simple check for $sum\_small\_block(S_{j}, g, 0, 0) = d'$.

\paragraph{Range Operations}

The range operations works similarly, they traverse the tree up gathering information on the way, and then descend down in case of range select operations.
If the query is fully contained within a small block, we get the answer from a lookup table and end.

First we find the lowest common ancestor $a$ of the leaves $u = b(i)$ and $v = b(j)$.
We keep two lists of partial answers, one for each branch starting at $u$ and $v$.

We first ask the small blocks for their answers and remember them in their respective lists.
In a loop until $u$ becomes $a$ we get the answers from the right siblings of $u$ and the left siblings of $v$, we remember them in the lists and move to their parents.
In the last iteration the left siblings and right siblings overlap; therefore we store the answers in only one of the lists.
Note that each list contains at most $c c'$ values as the table look-ups aggregate the answers.

We combine the lists into a single list and find the answer to the queries in it.
For $rmq$ we return the minimum of the list.
For $rmq\_size$ we sum $ms$ for those nodes whose minimum is equal to $rmq$.
In case of $rmq\_select$ we find the first node in the list such that the prefix sum of $ms$ is greater or equal to $n$.
We descend in such node to a child whose left siblings including itself sum up to $n$, and repeat until we get to a leaf.
Once we are in a leaf, we use a look-up table to solve the select there.

By the same argument as in search operations, the running time is $O(2 c c') = O(1)$.

\begin{algorithmic}
\Function{rmq\_list}{$S, g, i, j$} \Comment{$b(i) \ne b(j)$}
	\State $left \gets []; right \gets []$ \Comment{Initialization of the empty lists}
	\State $left.append(\{node{:}\ b(i), m{:}\ rmq\_small\_block(S_{b(i)}, g, i \% b, b - 1) + e[b(i) - 1],\allowbreak ms{:}\ rmq\_size\_small\_block(S_{b(i)}, g, i \% b, b - 1), M {:}\ \ldots, Ms{:}\ \ldots \})$
	\State $right.prepend(\{node{:}\ b(j), m{:}\ \ldots, ms{:}\ \ldots, M{:}\ \ldots, Ms{:}\ \ldots\})$
	\State $a \gets lca(u, v)$
	\While{$parent(u) \ne a$} \Comment{$u, v$ in different subtrees}
		\State $left.append(rmq\_info(S, g, u, "right"))$
		\State $right.prepend(rmq\_info(S, g, v, "left"))$
	\EndWhile
	\State $left.append(rmq\_info(S, g, u, v))$ \Comment{Between $u$ and $v$}
	\State $left.append(right)$
	\State \Return{$left$}
\EndFunction
\end{algorithmic}

The function $rmq\_info$ returns list of touples containing: node number, minimum, maximum, number of minima, number of maxima.
It processes the siblings of given vertex from left to right producing aggregated info per each $\frac{k}{c'}$ processed siblings.

\begin{algorithmic}
\Function{rmq\_block}{$S, g, i, j$}
	\If{$b(i) = b(j)$}
		\State \Return{$rmq\_small\_block(S_{b(i)}, g, i \% b, j \% b) + e[b(i) - 1]$}
	\Else
		\State $list \gets rmq\_list(S, g, i, j)$
		\State $m \gets \infty$
		\ForAll{$I \gets list$}
			\State $m \gets \min(m, I.m)$
		\EndFor
		\State \Return{$m$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{rmq\_size\_block}{$S, g, i, j$}
	\If{$b(i) = b(j)$}
		\State \Return{$rmq\_size\_small\_block(S_{b(i)}, g, i \% b, j \% b)$}
	\Else
		\State $list \gets rmq\_list(G, i, j)$
		\State $m \gets rmq\_block(G, i, j)$
		\State $ms \gets 0$
		\ForAll{$I \gets list$}
			\If{$I.m = m$}
				\State $ms \gets ms + I.ms$
			\EndIf
		\EndFor
		\State \Return{$ms$}
	\EndIf
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{rmq\_select\_block}{$S, g, i, j, n$} \Comment{Assuming $n$-th min exists}
	\If{$b(i) = b(j)$}
		\State $p \gets rmq\_select\_small\_block(S_{b(i)}, g, i \% b, j \% b, n)$
		\State \Return{$b(i) b + p$}
	\Else
		\State $list \gets rmq\_list(G, i, j)$
		\State $m \gets rmq\_block(G, i, j)$
		\State $ms \gets 0$
		\ForAll{$I \gets list$}
			\If{$I.m = m$}
				\State $ms \gets ms + I.ms$
				\If{$ms \ge n$}
					\State \Break
				\EndIf
			\EndIf
		\EndFor
\algstore{rmqselect}
\end{algorithmic}

Now $I$ contains aggregated information for up to $\frac{k}{c'}$ nodes.
Using a table look-up we find the correct node $p$ and also a new $n$ which applies to the node $p$.

\begin{algorithmic}
\algrestore{rmqselect}
		\State $(p, n) \gets table_1[left\_siblings\_inclusive(I.node), ms - n]$
		\While{$\boolnot is\_leaf(p)$}
			\State $(p, n) \gets table_1^*[children(p), n]$
		\EndWhile
		\State $x \gets max(i, p b) \% b; y \gets min(j, (p+1) b -1) \% b$
		\State \Return{$b(p)b + rmq\_select\_small\_block(S_{p}, g, x, y)$}
	\EndIf
\EndFunction
\end{algorithmic}

There are 8 look-up tables necessary for traversal and processing the tree.
The tables have an additional (non-mentioned) parameter which restricts the size of the block to be processed.
\begin{itemize}
	\item $fwd\_search$ -- looking for the first occurrence of $v$ in block of consecutive nodes.
	$bwd\_search$ requires another table returning the position of the last occurrence $v$.
	\item $rmq\_info$ -- aggregates info from consecutive nodes; four tables are necessary in total.
	\item $rmq\_select$ -- looking for $n$-th minimum returning index of the node among its siblings and number of minima in its preceding siblings.
	$RMQ\_select$ requires a similar table for maxima.
\end{itemize}
All of them work on blocks of $b$ bits with $O(1)$ parameters of size $O(\log k^c) = O(\log \log N)$.

\subsubsection{Macro Structure}

The macro structure starts with five arrays which summarize the results from the underlying blocks (root nodes of their min-max trees).
\begin{itemize}
	\item $e[i]$ --  the value at the end of the block $i$;
	In comparison to the block level, here we store the absolute values (the same for minima and maxima).
	Each element has size of $O(\log N)$ bits.
	\item $m[i]$, $M[i]$ -- the minimum and maximum values of the block $i$.
	\item $ms[i]$, $Ms[i]$ -- the number of occurrences in the block $i$.
	Each element is of size $O(\log\log N)$ bits.
\end{itemize}

First we deal with $sum$.
We use $B(i) = \lfloor\frac{i}{B}\rfloor$ to refer to the index of a block containing $i$.
The algorithm is very similar to $sum\_small\_block$.

\begin{algorithmic}
\Function{sum\_block}{$S, g, i, j$}
	\State $x \gets e[B(i - 1) - 1] + sum\_block(S_{B(i - 1)}, g, 0, (i - 1) \% B)$
	\State $y \gets e[B(j) - 1] + sum\_block(S_{B(j)}, g, 0, j \% B)$
	\State \Return{$y - x$}
\EndFunction
\end{algorithmic}

\paragraph{A structure for the Search Operation}

For each block $i$ we define the arrays of \emph{left-to-right minima} $lrm(i)$ such that $lrm(i)[0] = i, lrm(i)[j] < lrm(i)[j+1], m[lrm(i)[j]] < m[lrm(i)[j+1]]$.
Similarly we define left-to-right maxima arrays $LRM(i)$.

We can use the $lrm$ and $LRM$ arrays to implement $fwd\_search(S, g, i, d)$.
Let's assume that the result is not in block $B(i)$, if it was, we can solve it on the block level.
We look for a value $v = sum(S, g, 0, i) + d$ in the block $B(i) + 1$ using the min-max tree on the block level to find the answer.
If the answer exists in the block $B(i)+1$, we return the position and end.
Otherwise, either $v > M[B(i) + 1]$ or $v < m[B(i) + 1]$; in the first case we follow with the search in $lrm$, else in $LRM$.
We either find a block $j$ which contains the answer, and we finish the operation on the block level, or report a failure.

\begin{algorithmic}
\Function{fwd\_search}{$S, g, i, d$}
	\State $p \gets fwd\_search\_block(S_B(i), g, i \% B, d)$
	\If{$p \ne -1$}
		\State \Return{$B(i) B + p$}
	\Else
		\State $v \gets sum(S, g, 0, i) + d$
		\State $n \gets B(i) + 1$
		\If{$v < m[n]$}
			\State $j \gets lrm\_search(n, v)$
		\ElsIf{$v > M[j]$}
			\State $j \gets LRM\_search(n, v)$
		\Else
			\State $j \gets n$
		\EndIf
		\If{$j = root$}
			\State \Return{$-1$}
		\Else
			\State $d' \gets v - sum(S, g, 0, B(j) B - 1)$
			\State \Return{$B(j) B  + fwd\_search\_block'(S_{B(j)}, g, 0, d')$}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

Note that the final $fwd\_search\_block'$ is the standard $fwd\_search\_block$ altered to allow the answer $0$.

\bigskip

It remains to solve the $lrm\_search(n, v)$ in constant time ($LRM_search$ is similar).
Every node has at most $\frac{N}{B}$ ancestors; also for every node $i$ (except for the root) $m[i] - m[j] \le N$.
We could try to encode each array $lrm(i)$ as a bit string $lrm_i$ of size $N$ such that it has ones at positions $m[i] - lrm(i)[j] \forall j \ge 0$.
Then we could use the $pred_1(lrm_n, m[n] - v)$ operation to find a node $j$ which contains the value $v$.
However such representation leads to $O\left(\left(\frac{N}{B}\right)^2\right)$ elements in all $lrm$ arrays combined.

We observe that $lrm$ arrays are alike.
\begin{lemma}
	Let block $a$ be in two different sequences $lrm(i): lrm(i)[x_i] = a$ and $lrm(j): lrm(j)[x_j] = a$, then all following values in the sequence are the same: $lrm(i)[x_i+k] = lrm(j)[x_j+k] \forall k \ge 0$.
\end{lemma}

We use $lrm$ to construct a tree $T_{lrm}$ which is a trie of reversed $lrm$ sequences with an artificial root with assigned minimum $m[root] = -\infty$.
The properties of the tree are:
\begin{enumerate}
	\item $m[i] > m[parent(i)]$;
	\item $i < parent(i)$;
	\item the tree has $\frac{N}{B} + 1$ nodes.
\end{enumerate}

The search for a value $v$ in $lrm(n)$ is transformed to a search for an ancestor $n'$ of $n$ in the tree $T_{lrm}$ such that $m[n'] \le v$.
We split the search into two parts:
\begin{enumerate}
	\item a search in $lrm$ restricted to powers of two;
	\item a search in $lrm$ with a bounded distance.
\end{enumerate}

We reduce the $lrm$ arrays to contain at most $\log N$ elements each.
$$lrm'(i)[j] = lrm(i)[2^j] \forall j \ge 0$$
Then we represent the arrays as bit strings as we proposed earlier.
We also store an array of at most $\log N$ elements translating $j$-th one to a node number.

After the first jump we have an estimate of depths and heights and also a bound on number of ancestors which we have to search through.
\begin{align*}
depth(n'') - depth(n') &< depth(n) - depth(n'') \\
height(n'') &\ge depth(n) - depth(n'') \\
depth(n'') - depth(n') &\le height(n'')
\end{align*}

We decompose the tree iteratively to paths; in each step the longest path $p$ is removed.
\begin{align*}
height(start(p)) = length(p)
\end{align*}
We extend the path $p$ into a ladder $l$ by including $length(p)$ of ancestors of $start(p)$.
With the previous bound, we are guarantied that $n'$ is in the same ladder as $n''$.
There are as many ladders as leaves of the tree $T_{lrm}$.
All ladders combined contain $O\left(\frac{N}{B}\right)$ nodes.

For each node of the tree, we have a mapping of its number to the representation of the ladder it belongs to and its position withing the ladder.
The ladder is a bit string $ladder_l$ of size $N$ containing $length(l)$ ones at positions $m[end(l)] - m[l[j]] \forall j \ge 0$ (the leaf is at position $0$).
In a ladder we perform a successor search for a value $m[end(l)] - v$ and translate the position back to node number.

\begin{algorithmic}
\Function{lrm\_search}{$n, v$}
	\State $n'' \gets ancestor_1[n][rank_1(pred_1(lrm_n, m[n] - v))]$
	\State $(l, p) \gets ladder[n'']$ \Comment{Returns index of ladder and position within}
	\State $n' \gets ancestor_2[l][rank_1(succ_1(ladder_l, m[end[l]] - v))]$
	\Return{$n'$}
\EndFunction
\end{algorithmic}

There are several bit strings and tables:
\begin{itemize}
	\item $lrm_n$ -- a collection of $\frac{N}{B}$ bit strings of size $N$ containing $\le \log N$ ones. \todo{structure}
	\item $ancestor_1$ -- a collection of $\frac{N}{B}$ tables mapping $i$-th ancestor in $lrm'$ to a node number.
	Each of $\log N$ elements has size $\log N$; the total space is therefore $\frac{N}{B} \log^2 N = o(N)$ as long as $c \ge 3$.
	\item $ladder$ -- a table mapping node number to ladder index and position within.
	It contains $\frac{N}{B}$ elements of size $\log{N}$.
	\item $ladder_l$ -- a collection of up to $\frac{N}{B}$ bit strings of size $N$ containing in total $2 \frac{N}{B}$ ones.
	\item $end$ -- maps ladder index to a node number of its leaf.
	It contains $\le \frac{N}{B}$ elements of size $\log{N}$.
	\item $ancestor_2$ -- a collection of $\le \frac{N}{B}$ tables which for a given ladder index translates the position in the ladder to a node number.
	It contains $\le \frac{N}{B}$ elements of size $\log{N}$.
\end{itemize}

\paragraph{Range Queries}

We can use the structure from lemma \ref{lemma:rmq2} with a minor changes (it works with values rather than positions) to solve $rmq$ and $RMQ$.
This time it is sufficient to use it only once provided that we set $c \ge 3$.

The structure for $rmq$ is however not suitable for querying the total number of occurrences of minimum nor selecting $n$-th of such.
If we augmented each precomputed interval of size $2^k$ with the number of occurrences, we would still fit in the same space, however computing the number of occurrences cannot easily combine information from only two intervals like the $\min$ function does.
In this case, we have to use the inclusion-exclusion principle: adding the numbers of occurrences in both intervals and subtracting the number of occurrences in the overlapping part.
The size of the overlapping part is not a power of two in general and so, its number of occurrences is not precomputed, which leads to a recursion and running time $O(\log N)$.

\paragraph{Bitmaps of Minima and Maxima}

For each minimum $r$ (and maximum $R$, but we formulate the structure for minima only as the maxima case is symmetric) we define two bit strings:
\begin{itemize}
	\item $p_r[i] = (m[i] = r)$; $p_r$ is a bitmap which marks blocks containing the minimum $r$.
	For purposes of $rmq\_size$ and $rmq\_select$ only these are interesting because the other blocks either do not contain the minimum $r$ (if their minimum is $> r$) or cannot be accounted (if their minimum $r' < r$, then a query spanning over this block has a minimum $r'$ rather than $r$).
	Each bit string has size $N$ and contains $k_r$ ones (which is number of blocks with such minimum).
	Because $p_r$ is a partitioning of $[0, N - 1]$, $\sum{r} k_r = \frac{N}{B}$.
	\item $s_r$ from the lemma \ref{l:sps} applied on numbers of occurrences of the minimum $r$ in blocks.
	The bit string has size up to $N$ (we extend it to $N$) and contains $k_r$ ones.
\end{itemize}
There exist at most $\frac{N}{B}$ distinct minima and therefore the same number of bit strings.

Using these two sets of arrays, we can support $rmq\_size$ and $rmq\_select$.
It only necessary for us to focus on the span of the query as we the prefix and suffix can be handled using the min-max trees on the block level.
Without loss of generality we assume that the query has only span.

\begin{algorithmic}
\Function{rmq\_size}{$S, g, i, j$}
	\State $r \gets rmq(S, g, i, j)$
	\State $y \gets rank_1(p_r, B(j)); x \gets rank_1(p_r, B(i) - 1)$
	\State \Return{$prefix\_sum(s_r, y) - prefix\_sum(s_r, x)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{rmq\_select}{$S, g, i, j, n$}
	\State $r \gets rmq(S, g, i, j)$
	\State $s \gets prefix\_sum(s_r, rank_1(p_r, B(i) - 1))$ \Comment{Prefix sum before $i$}
	\State $a \gets select_1(p_r, prefix\_leq(s_r, s + n))$ \Comment{Block containing the $n$-th minimum}
	\State $n' \gets s + n - prefix\_sum(s_r, a - 1)$ \Comment{$n$-th within block $a$}
	\State \Return{$B a + rmq\_select\_block(S_a, 0, B - 1, n')$}
\EndFunction
\end{algorithmic}

In the general case, the minimum $r$ might have occurred only in prefix or suffix and not span; then the bit string $m_r$ does not exist and we simply skip the span.
In case of select, we first check using $rmq\_size$ in which part the answer lays and then call $rmq\_size$ on that part with altered $n$ by the number occurrences in the preceding parts.

\iffalse

It remains to show that the bit strings do not require too much information.

\begin{lemma}
	The function $f(k) = \log {M \choose k}$ is concave, for any positive constant $M$ and $0 \le k \le M$.
\end{lemma}
\begin{proof}
	In case of discrete functions, the condition of convexity can be equally expressed using the previous and next values for $k \in [0, M]$.
	\begin{align*}
		f(k) + f(k) &\ge f(k-1) + f(k+1) \\
		\log {M \choose k}^2 &\ge \log {M \choose k - 1} {M \choose k + 1} \\
		\left(\frac{M!}{k! (M-k)!}\right)^2 &\ge \frac{M!}{(k - 1)! (M - k + 1)!} \frac{M!}{(k + 1)! (M - k - 1)!} \\
		1 &\ge \frac{1}{(M-k+1)(k+1)}
	\end{align*}
\end{proof}

\begin{lemma}
	The amount of information contained in the all arrays $p_r$ and $s_r$ combined is $o(N)$ bits.
\end{lemma}
\begin{proof}
	We use Jensen's inequality for the function $f$ with $M = \frac{N}{B}$ applied on the bit strings $p_r$.
	Without loss of generality, we can assume that there are exactly $\frac{N}{B}$ bit strings; if there are less of them, the rest will be empty.
	\begin{align*}
		f(1) = f\left(\frac{\sum_i k_i}{\frac{N}{B}}\right) &\ge \frac{\sum_i \log {\frac{N}{B} \choose k_i}}{\frac{N}{B}} \\
		o(n) = O\left(\frac{N}{B} \log \frac{N}{B}\right) = \frac{N}{B} f(1) &\ge \sum_i \log {\frac{N}{B} \choose k_i}
	\end{align*}
	
	Similarly we use follow with the bit strings $s_r$; this time $M = N$.
	\begin{align*}
		f(1) = f\left(\frac{\sum_i k_i}{\frac{N}{B}}\right) &\ge \frac{\sum_i \log {N \choose k_i}}{\frac{N}{B}} \\
		o(n) = O\left(\frac{N}{B} \log N\right) = \frac{N}{B} f(1) &\ge \sum_i \log {N \choose k_i}
	\end{align*}
\end{proof}

Note that the split into $p_r$ and $s_r$ is necessary.
If we had only $p_r$ of size $N$ and somehow represented counts instead of occurrence of minimum, the Jensen's inequality would lead to space $N \log N$.
If we kept only the bit string $s_r$ (and dealt with zeros), the upper bound would be $\left(\frac{N}{B}\right)^2 \log B$.

\fi

We represent the collections of bit strings $p_r$ and $s_r$ using the previous lemma. \todo{structure}
The space complexity of $p_r$ is $O\left(\frac{N}{B} \log \frac{N}{B}\right) = o(N)$; in case of $s_r$ the space complexity is $O\left(\frac{N}{B} \log N \right) = o(N)$.

\subsection{$\pm 1$ Functions Revisited}

Although there are 9 functions $g$ mapping ${0, 1} \to {-1, 0, 1}$, we only use 3 ($\pi, \phi, \psi$).
Answers to all operations except for searches with parameters $\phi$ and $\psi$ can be answered using the structure for $\pi$.

For the operation $sum$, the following identities hold:
\begin{gather*}
	sum(S, \pi, i, j) = sum(S, \phi, i, j) - sum(S, \psi, i, j) \\
	sum(S, \phi, i, j) + sum(S, \psi, i, j) = j - i + 1
\end{gather*}

We can find the explicit formula for either of them:
\begin{align*}
	sum(S, \phi, i, j) &= \frac{(j - i + 1) + sum(S, \pi, i, j)}{2} \\
	sum(S, \psi, i, j) &= \frac{(j - i + 1) - sum(S, \pi, i, j)}{2}
\end{align*}

Because the sums of $g \in \{\phi, \psi \}$ are monotonous, we can reduce all range queries to sums, searches and arithmetics.
In the range $i, j$, the minimum occurs for the first time at $i$ and maximum occurs the last time at $j$.
All occurrences are continuous.
\begin{align*}
	rmq(S, g, i, j) &= sum(S, g, 0, i) \\
	RMQ(S, g, i, j) &= sum(S, g, 0, j) \\
	rmq\_size(S, g, i, j) &= \min(j + 1, fwd\_search(S, g, i, 1)) - i \\
	RMQ\_size(S, g, i, j) &= j - \max(i - 1, bwd\_search(S, g, j, 1)) \\
	rmq\_select(S, g, i, j, k) &= i + k - 1 \\ 
	RMQ\_select(S, g, i, j, k) &= j - k + 1
\end{align*}

\subsubsection{Search Operations}

The only non-trivial operations are $fwd\_search$ and $bwd\_search$.
They cannot be reduced and must be supported extra.

\todo{phi and psi search operations}

\subsection{Extension of BP}

We focus on encoding the tree as balanced parenthesis, however instead of the traditional indices for rank and select, we use the new data structure.
We have already shown how to reduce all old primitive operations to the new one, which implies that everything shown in the section \todo{ref} is support here too.
Because sum, search, and range operations are more general, we can support more operations.

Child operations -- $degree$, $child_rank$ and $child_select$, which are important for basic navigation in the tree, were only available with a specialized index.
They were also the reason for using DFUDS representation.
Using the generalized $rmq$, we present their simple implementation.
The key observation is that in a representation of a subtree of vertex $i$ with omitted terminal parenthesis, the occurrences of minimum correspond to the terminal parenthesis of the children of $i$.

\begin{algorithmic}
\Function{degree}{$i$}
	\State \Return{$rmq\_size(S, \pi, i, find\_close(i) - 1)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{child\_rank}{$i$}
	\State $p \gets parent(i)$
	\State \Return{$rmq\_rank(S, \pi, p, find\_close(p), find_close(i))$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{child\_select}{$i, n$}
	\State \Return{$find\_open(rmq\_select(S, \pi, i, find\_close(i), n))$}
\EndFunction
\end{algorithmic}

Level ancestor operation is a straightforward generalization of enclose.

\begin{algorithmic}
\Function{level\_ancestor}{$i, d$}
	\State \Return{$bwd\_search(S, \pi, i, d + 1)$}
\EndFunction
\end{algorithmic}

We can also support local navigation on vertices on the same level $l$ within a restricted subtree rooted in a vertex $r$.

\begin{algorithmic}
\Function{level\_first}{$r, l$}
	\State \Return{$fwd\_search(S, \pi, r, l - depth(r))$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{level\_last}{$r, l$}
	\State \Return{$find\_open(bwd\_search(S, \pi, find_close(i), l - depth(r))$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{level\_next}{$r, i$}
	\State \Return{$fwd\_search(S, \pi, find\_close(i), 0)$}
\EndFunction
\end{algorithmic}

\begin{algorithmic}
\Function{level\_prev}{$r, i$}
\texttt{}	\State \Return{$find\_open(bwd\_search(S, \pi, i, 0))$}
\EndFunction
\end{algorithmic}

\bigskip

The only operations which are not supported by this structure are: $level\_size$, $level\_rank$, $level\_select$, $lo\_rank$, and $lo\_select$.
All of them would require a better handling of the level structure, something like $fwd\_search$ with parametrized occurrence.
If these operations are crucial, LOUDS representation supports them.

\section{TC}

All structures which we have seen have several things in common; they are combined from two or three levels of different representations:
\begin{itemize}
	\item On the lowest level they have small blocks of size less than $\log n$ bits.
	With such size, it is possible to precompute results to all possible queries which are contained in the small block.
	\item Then an intermediate level follows which connects several blocks together to blocks of size $\log^c n$ bits.
	It often uses the property that the pointers withing such block require only $O(\log \log n)$ bits.
	\item Finally, the macro level which spans the whole structure connects individual blocks.
	It usually uses a classical non-succinct structure provided that it requires only $O(n \log^{c'} n)$ bits where $c' < c$.
\end{itemize}

So far the small blocks and blocks were chunks of a bit string representation of the tree.
They were designed with the regular size and offset in one's mind instead of being connected to the structure of the tree.
The representation of a single vertex, let alone the sequence of its children, was covered by multiple different (small) blocks.

The tree covering approach respects the structure of the tree by its decomposition into components of a bounded size.
Not dissimilar to the other representations, it also uses three levels and with bit strings on the lowest one and a pointer structure on the top one.

\subsection{Decomposition Algorithm}

Several decomposition algorithms have been proposed; they are parametrized by a target number of vertices $B$ to be present in a component.
The first algorithm \todo{ref} decomposed the tree into components of size $[B, 3 B - 2]$ with the exception of the component containing the root, which could have been undersized.
Such decomposition cannot exist unless we allow the components to overlap; more specifically to overlap in their common root.
From the bounds, it is clear that there exist $O(\log n)$ components.

This decomposition was used and lead to a succinct data structure which at the time of its introduction supported more operations than BP (child operations) or DFUDS (depth).
The problem of the decomposition was that the components were connected together in too many ways and the structure had to handle many cases.

Later a different decomposition was proposed which removes the lower bound on the sizes of the components while retaining their asymptotic number in exchange for restriction on how components can be connected with each other.
There can be at most one edge per component which connects it to a vertex in a different component.
Note that a stronger claim of no such edge existing does not provide a decomposition in a general case (e.g. a tree with depth greater than $2 B$).

We call a vertex \emph{heavy} if its subtree contains at least $B$ vertices.
All ancestors of a heavy vertex are heavy and therefore heavy vertices form a subtree $T_{heavy}$ of the tree $T$.
We call a heavy vertex \emph{branching vertex} if it has at least two heavy children, and \emph{branching edges} are the edges from a branching vertex to its heavy children.

\begin{lemma}
	There are $O\left(\frac{n}{B}\right)$ branching vertices and branching edges in a tree $T$.
\end{lemma}
\begin{proof}
	The tree $T_{heavy}$ has at most $\frac{n}{B}$ leaves (we call them \emph{heavy leaves}) as each leaf is a root of a subtree of $T$ which contains at least $B$ vertices.
	Each branching node connects at least two heavy subtrees containing each at least one heavy leaf; therefore their number must be less than the number of all heavy leaves.
	The number of branching edges is the same as the number of heavy leaves plus number of branching vertices minus one which can be seen after contracting non-branching edges in the tree $T$.
\end{proof}

The algorithm works in DFS post-order; it first recursively processes all children of a vertex $v$ before solving $v$ itself.
A leaf starts its own temporary component.
From the recursion a set of \emph{permanent} and up to one \emph{temporary} component is returned.
The way how they are merged together depends on number of heavy children.

We distinguish three cases:
\begin{enumerate}
	\item If a vertex $v$ does not have any heavy children, then all children are part of a temporary component.
	Children are processed from left to right; their temporary components are merged with $v$ and potentially with components of their right siblings.
	When the size of a component is at least $B$, we declare it permanent.
	If at least one component was declared permanent, we declare all of them as permanent even though the last one can be undersized.
	\item If a vertex $v$ has exactly one heavy child $u$, then we process it in a similar way as the case (1).
	If $u$ is part of a temporary component, nothing changes; otherwise we simply skip it.
	\item If a vertex $v$ has two or more heavy children (it is a branching vertex), the temporary components containing the heavy children are declared permanent no matter what size they have.
	All non-heavy children are split into intervals by the heavy children and processed as in case (1) while declaring all their components as permanent.
	If the vertex $v$ does not have any non-heavy children, then it forms a permanent component of size one.
\end{enumerate}

Several invariants hold during the course of the algorithm:
\begin{itemize}
	\item Size of a permanent component is less than $2 B$; size of a temporary component is less than $B$.
	A temporary component is only merged with other temporary components; it is declared permanent when its size is at least $B$.
	Because we merge a temporary component with the parent first and then with its right siblings one by one, its size will never be greater than $(B - 2) + 1 + (B - 1)$, at which point it is declared permanent.
	\item If a vertex is shared among multiple components, it is their root.
	When such situation happens in the algorithm, all of them are declared as permanent and they are never dealt with again.
	\item There is at most one edge leaving a component from a non-root vertex.
	The vertex $w$ connected with such an edge to a component with root $u$ is heavy.
	By contradiction, let it be a non-heavy vertex which is a root of a permanent component.
	In case (2) and (3) only permanent components involving a heavy vertex are created.
	In case (1) an undersized permanent component can be created, however a regular permanent component is created as well.
	The subtree size is therefore at least $B$, a contradiction.
	As a consequence, $u$ is heavy as it is an ancestor of $w$.
	
	Let's assume that it holds for all children, then it holds after the merge at vertex $v$ too.
	We are dealing with a temporary component of vertex $u$ and a permanent component of $w'$ which we want to connect to it.
	Both $u$ and $w'$ are heavy, so case (3) applies; the temporary component is not changed and immediately declared permanent.
	\item The number of component is $O\left(\frac{n}{B}\right)$.
	An undersized component was declared permanent in (1) and (2) only when another component of a regular size was declared permanent; in (3) when it was connected with a branching edge, or once per interval of non-heavy children which was either separated by a branching edge on the left or it is the first interval of a branching vertex or it is the branching vertex itself.
	From that the bound follows.
\end{itemize}

\begin{algorithmic}
\Function{Decompose}{$v, B$}
	\If{$is\_root(v)$}
		\State \Return{$\O, \{v\}$} \Comment{No permanent components, itself as temporary}
	\Else
		\State $P \gets \O; T \gets []; h \gets 0$
		\ForAll{$u \gets children(v)$}
			\State $(p, t) = Decompose(u)$ \Comment{Process child}
			\State $P \gets P \cup p; T.append(t)$ \Comment{Gather permanent and temporary}
			\If{$size(u) \ge B$} \Comment{Count number of heavy children}
				\State $h \gets h + 1$
			\EndIf
		\EndFor
		\State
		
		\State $s \gets \{v\}; i \gets 0$ \Comment{Working component, }
		\ForAll{$(u, t) \gets zip(children(v), T)$}
			\If{$size(u) \ge B \booland t \ne \O \booland h > 1$} \Comment{Heavy, temporary, case 3}
				\State $P \gets P \cup \{t\}; t \gets \O$ \Comment{Declare permanent}
			\EndIf
			\If{$t = \O \booland |s| > 1 \booland h > 1$} \Comment{Permanent, non-empty $s$, case 3}
				\State $P \gets P \cup \{s\}; s \gets \{v\}$  \Comment{Declare permanent, reset}
			\ElsIf{$t \ne \O$} \Comment{Temporary}
				\If{$|s| + |t| \ge B$} \Comment{Regular sized}
					\State $P \gets P \cup \{s \cup t\}; s \gets \{v\}; i \gets i + 1$ \Comment{Declare permanent}
				\Else
					\State $s \gets s \cup t$ \Comment{Merge $t$ into $s$}
				\EndIf
			\EndIf
		\EndFor
		\State
		
		\If{$|s| = 1$}
			\State $s \gets \O$ \Comment{Reset before returning}
		\ElsIf{$i \ge 1 \boolor h > 1$} \Comment{At least one regular sized or case 3}
			\State $P \gets P \cup \{s \cup t\}; s \gets \O$ \Comment{Declare permanent}
		\EndIf
		\If{$h = degree(v) \booland h > 1$} \Comment{All are heavy, case 3}
			\State $P \gets P \cup \{v\}$ \Comment{Declare $v$ as permanent}
		\EndIf
		
		\State \Return{$P, s$}
	\EndIf
\EndFunction
\end{algorithmic}

\subsection{The Structure}

