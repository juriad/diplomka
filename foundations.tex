\chapter{Foundations}

\section{Model of Computation}

We state all our results for \emph{word-RAM}, a version of \emph{Random Access Machine}, which resembles current computers.
A \emph{word} is a sequence of $w$ consecutive bits which can be accessed and processed in one step of the computation.
The constant $w$ depends on the space complexity $S$ in words of the problem which is being solved.
A lower bound of $w \ge \lceil \log S \rceil$\footnote{
	All logarithms are binary. 
	We will usually assume the ceiling function of the logarithm while not explicitly writing it.
	This does not change the asymptotic time nor space complexity of the algorithms as it is only a lower order term.
}
 is necessary in order to allow access to any piece of memory which is used during the computation.
We usually assume that $w = \Theta(\log S)$ as working with larger words would make the model too powerful.

\bigbreak

There are many definitions of word-RAMs, therefore we briefly describe the one which we are going to use.

RAM has access to two types of memory:
\begin{itemize}
	\item \textbf{Main memory} which forms a sequence of words, which are the smallest addressable units.
	The sequence is potentially unbounded, however only a limited number of words can be accessed, depending on the word size.
	
	We assume (as in case of Turing machine), that the input is written in the main memory starting at position $0$.
	The input can be encoded in various ways, for example prefixed with the number of words it requires.
	Except for the input, the memory is filled with zeros when the program starts.
	
	\item Constant number of \textbf{registers}; each of them stores a single word.
\end{itemize}

The instructions supported by the word-RAM are:
\begin{description}
	\item[Flow control]
	Instructions which are used for determining which instruction to process next.
	\begin{description}
		\item[Termination: \textnormal{$\texttt{halt}$}]
		Stops the program immediately.
		\item[Jumps: \textnormal{$\mathit{label}{:}\ \mathit{instruction}, \texttt{goto}\ \mathit{label}, \texttt{goto}\ \mathit{label}\ \texttt{if}\ a$}]
		Continues with execution of the instruction at the specified label.
		In the latter case, it goes to the label only if $a$ is $\true$, otherwise it continues with the next instruction.
		\item[Functions: \textnormal{$\texttt{function}\ \mathit{fn}(\mathit{args}\ldots), \texttt{return}\ a$}]
		Defines a function $\mathit{fn}$ with a constant number of arguments, possibly zero.
		A function has access to the main memory but not to any registers which were used by its caller.
		Each function ends with a return instruction followed by a value.
		After a return, the program continues with an instruction which follows the one which initiated the call.
	\end{description}
	
	\item[Memory access]
	Instructions for accessing and modifying the main memory.
	\begin{description}
		\item[Read: $\mathit{register} \gets \mathit{Memory}[\mathit{position}\char93$]
		Reads a word from the main memory and stores it in a named $\mathit{register}$.
		\item[Write: $\mathit{Memory}[\mathit{position}\char93 \gets a$]
		Overwrites the word stored in the main memory at the given $position$ with the value $a$.
	\end{description}
	
	\item[Value manipulation]
	Instructions on values which are stored registers; the resulting value is always stored to a register: $\mathit{register} \gets \mathit{operation}$.
	\begin{description}
		\item[Arithmetic: $a + b, a - b, a \cdot b, a \mathbin{/} b, a \% b$]
		All overflows are silently discarded; the division is integral.
		\item[Bitwise: $a \bitand b, a \bitor b, a \bitxor b, \bitnot a, a \bitlsh b, a \bitrsh b$]
		Bit shifts are padded with zeros.
		\item[Boolean: \textnormal{$a \booland b, a \boolor b, \boolnot a$}]
		Integers are implicitly converted to boolean ($0 \Leftrightarrow \false$), and booleans to integers ($\true \Rightarrow 1; \false \Rightarrow 0$).
		\item[Relational: $a < b, a \le b, a > b, a \ge b, a = b, a \ne b$]
		The result is a boolean value.
		\item[Function call: $\mathit{fn}(\mathit{args}\ldots)$]
		Calls a function $\mathit{fn}$, provides it required number of arguments.
		The result is the value returned by the function.
	\end{description}
\end{description}
All single letter arguments can be either names of the registers or constants of size $w$ bits.

\subsection{RAM for Bit Strings}

The general word-RAM is too restrictive for us as we will need to address individual bits or ranges of bits which are not aligned with words in the main memory.

\begin{lemma}
	We redefine the operations Read and Write to reflect our needs.
	
	\begin{description}
		\item[Read: $\mathit{register} \gets \mathit{Memory}[\mathit{start}:\mathit{end}\char93$]
		Reads the $\mathit{end} - \mathit{start} \le w$ bits from the main memory starting with the bit at position $\mathit{start}$, which is in $(\mathit{start} \% w)$-th word and stores them padded with zeros from left in a $register$.
		\item[Write: $\mathit{Memory}[\mathit{start}:\mathit{end}\char93 \gets a$]
		Overwrites the $\mathit{end} - \mathit{start} \le w$ bits stored in the main memory at the given starting position with the $\mathit{end} - \mathit{start}$ least significant bits of the value $a$.
	\end{description}
	
	We also redefine $\mathit{Memory}[\mathit{start}]$ to be equal to $\mathit{Memory}[\mathit{start}:\mathit{start} + 1]$.
\end{lemma}
\begin{proof}
	These redefined instructions can be emulated with a constant number of arithmetic, bitwise, and up to two original memory accessing or modifying instructions.
\end{proof}

We define the precise way how the input is stored in the main memory.
The first word (at position $0$) contains the total number of bits -- that can require up to $\log w S = \Theta(\log S)$ bits.
Starting with the position $1$ the bits of the input are stored consecutively filling the words from the most significant to the least significant bits.
For convenience of querying ranges past the end of the input, we keep the one word after the end of the input free.

We prefer to express the space complexities in bits rather than words and except for the very last part we will not use words at all.
All values that our algorithms will store in registers will fit into them.

\section{Space-Efficient Data Structures}

In a universe $U$, every object can be represented as a sequence of $\lceil \log |U| \rceil = N$ bits simply by ordering all objects in the universe and encoding their position within the order in binary.
Although this encoding is optimal in terms of the space used, it is rarely useful for more advanced applications than iterations over the objects in the universe and reconstruction of the original object.
Many operations cannot be supported unless the object is fully decoded into a more traditional form first; others can be processed on the encoded object, however their time complexity is much worse than optimal.

The goal of space-efficient data structures is to allow some additional space in exchange for more and faster operations.
There are three major representatives depending on their space complexity.

\begin{description}
	\item[Implicit data structure]
	An \emph{implicit data structure} for a universe $U$ is a data structure which has space complexity $ \log |U| + O(\log \log |U|) $ bits of space; sometimes only $O(1)$ extra bits is allowed.
	The encoding of the input is important in case of the tighter definition as it does not allow storing the size of the input itself.
	An example of an implicit structure is a C string of characters which is terminated by a zero byte.
	
	Another example\label{ex:implicit-graph} are general directed labeled graphs on $n$ vertices which can be stored in $n (n - 1)$ bits -- for pair of vertices $i \ne j$ a bit is stored whether the edge $(i, j)$ is in the graph or not -- this is basically adjacency matrix without its diagonal.
	The only supported operation in constant time is determining whether two vertices are connected by an edge.
	
	Only few implicit structures are known, since the restriction posed by the definition of an implicit data structure is often too strict.

	\item[Succinct data structures]
	A \emph{succinct data structure} is a data structure whose space complexity is $ \log |U| + o(\log |U|) $ bits.
	There exist succinct data structures for encoding some restricted classes of graphs, such as planar graphs, and trees.
	The rest of this thesis is about succinct structures.
	
	Extending our example of a general graph by an operation for querying out-degrees of vertices results in a succinct data structure.
	We simply add an array of precomputed out-degrees which requires $n \log n = o(n^2)$ bits.
	
	Moreover, there is a succinct structure which supports adjacency testing and iteration over the list of neighbors and more in constant time. \cite[Theorem~6.1]{raman2007succinct}
	
	\item[Compact data structure]
	A \emph{compact data structure} is a data structure which uses $ O(\log |U|) $ bits of space.
	
	A compact structure is often a byproduct of a succinct structure when the constants in space or time complexity prove to be too big for a real use. \cite{gonzalez2005practical}
	
	We can easily extend the succinct structure for directed graphs into a compact structure for undirected graphs by simply using it twice: for the original graph and the graph with reversed edges.
\end{description}

\bigbreak

In this thesis, we are only interested in succinct data structures.

\section{Storage}

There are two distinct approaches to representing data structures as bit strings depending on how the data are partitioned.

The \emph{systematic approach} splits the encoding into two parts:
\begin{itemize}
	\item the data sufficient for reconstruction of the structure.
	This could be an implicit structure, however a more verbose encoding is often used.
	For example in case of trees, it can be made by a traversal or a decomposition of the tree.
	It can often answer some local queries.
	\item auxiliary indices which help to speed up or even realize more advanced operations on the data structure.
	Indices store additional information which is intended to offer answers to queries which are non-local, such as depth of a vertex, or the lowest common ancestor.

	The size of the indices is dominated by the size of the data.	
	We call an index succinct if its size if $o(N)$.
	
	Sometimes it is easier to think about \emph{density} of an index (or its part) in bits per bit of data.
	The density of a succinct index is $o(1)$.
\end{itemize}

In systematic approach, it is easy to add another index and thereby support new operations.
It is also possible to drop indices which are not necessary for particular use-cases in order to make the encoded data structure a little smaller.
It can sometimes be beneficial to save only the data while recomputing all of the indices when the data structure is loaded into memory.

\bigbreak

The \emph{non-systematic encoding} cannot be split into such two parts.
The organization of data itself allows all kinds of operations to be supported in desired time.

Non-systematic representations are designed with the set of supported operations in one's mind.
It is not possible to remove support for any operation in order to spare some space.
Adding support of a new operation which was not planned beforehand requires its composition from the already supported operations.
It is also possible to add an additional index to a non-systematic data-structure, which results in a hybrid structure.

\bigbreak

Most of the presented results are about complex indices for simple encodings of trees.

\section{Basic Techniques}

There are a few techniques which are used in nearly every data structure.
The idea of most of them is simple, however when combined together they pose a strong tool.

\subsection{Division and Pointers}

Every data structure is only a string of bits and unless we can calculate offset in any other way (as in case of the implicit graphs \ref{ex:implicit-graph}), we need to divide the bit string into parts and use pointers to navigate between them.
In the following lemma we show that a small number of independent parts can be concatenated in one bit string.

\begin{lemma}\label{l:concat}
	Let $K = o(\frac{N}{\log N})$ be the number of bit strings $S_1, S_2, \ldots, S_K$ and $N_1, N_2, \dots, N_K$ their lengths which sum up to $N$.
	Then we can combine all bit strings into a single bit string $S$ while maintaining succinctness and constant-time access to the individual bit strings.
	Typically $K = O(1)$.
\end{lemma}
\begin{proof}
	The new bit string $S$ starts with the number of parts in binary followed by a header in form of a table and continues with concatenated bit strings $S_1, \ldots, S_K$ without any delimiters.
	The table contains $K$ offsets of all the original bit strings within $S$ ignoring the space taken by the header.
	Each offset is encoded as a binary number using $\lceil \log N \rceil$ bits, which results in the table having size $\log K + K \lceil \log (N) \rceil = o(N)$.
\end{proof}

This lemma is used whenever we add indices to a systematic succinct data structure, in such cases we use it as a fact and do not emphasize it.

We store an offset -- which we also call a pointer -- relatively to a certain position in the bit string rather than absolutely from its beginning.
The size of the offset in bits depends on the range into which it points.
The fact that the pointers can have different sizes allows us to decompose a big structure into small parts with restricted pointers between them.
There will be a lot of pointers within individual parts, however they require only a small size, and pointers between parts, which are large in size but used only seldom.

\subsection{Precomputation}

The goal of most succinct data structures is to perform queries in constant time.
Even if this was possible on high level, in the end the query must be processed on data of size close to a machine word.
The following lemma shows that we can precompute all queries restricted to a small block of data.

\begin{lemma}
	All answers to a query $Q$ with parameters of total size $P$ restricted to $B$ consecutive bits of memory with an answer of size $R$ bits can be solved in $O(1)$ time with an index of additional $2^{B+P} R$ bits.
	
	As long as $B + P + \log R < \log N$, this results in a succinct index.
\end{lemma}
\begin{proof}
	We represent this as a table of consecutively stored precomputed answers to $2^{B+P}$ possible queries.
	The index of the table is the $B$ bits of data concatenated with $P$ bits of parameters interpreted as a number.

	Lookup is easy: $B$ and $P$ bits are concatenated and multiplied by $R$, which gives the offset of the result.
	
	The bound for a succinct index follows from the total size of the table.
\end{proof}

We can say that a look-up table captures the result of an algorithm with parameters of size $P$ accessing $B$ consecutive bits of the main memory and returning a result of $R$ bits.

We call the structure from the lemma a look-up table; this technique is also known as the Method of Four Russians.
All our look-up tables will be succinct indices.

\bigbreak

The look-up tables are sometimes used merely as a way to overcome the differences between the word-RAM and the real computers as there are instructions in modern computers which cannot be computed directly on the word-RAM.

If the algorithm captured by the look-up table can be split into several parts, then a single large look-up table can be replaced with two or more look-ups in substantially smaller tables.
This can result in an implementation which is more considerate to caches in modern CPUs in exchange for a theoretical constant-time slowdown.

\subsection{\label{ss:index-without-data}Index Without Data}

Let's assume that we want to support an operation on derived data.
Instead of counting ones, as in the previous example, we want to count the transitions from zero to one in the bit string $S$.
We can define a new bit string $T$ which has a bit with value one for each such transition:
$$ T = \bitnot S \bitand (S \bitlsh 1)$$

On top of the bit string $T$ we build a succinct rank index.
Although the index is small, $o(N)$, we cannot afford to store the whole bit string $T$ since it would make our data structure compact and not succinct.

We solve this issue by keeping only the small index while dropping the bit string $T$.
Whenever the ranking algorithm accesses the bit string $T$, it can only request $w$ bits at a time, which we can compute on the fly with a constant slowdown from the bit string $S$ using the given formula.

This technique works in general; any algorithm on word-RAM can access at most $w$ bits of the input in each step.
We do not need to know how the operation works as long as we are able to provide it any chunk of $w$ bits of memory in constant time upon request.
