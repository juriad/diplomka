\chapter{Foundations}

\section{Model of Computation}

We state our results for a version of \emph{Random Access Machine}, which resembles current computers however in its definition it is better equipped for dealing with bit strings.

RAM has access to two types of memory:
\begin{itemize}
	\item \textbf{Main memory} which forms a sequence of bits.
	The sequence is potentially unbounded and fill with zeros when the program starts.
	
	The constant $w$ depends on the space complexity $S$ of the problem which is being solved.
	A lower bound of $w \ge \lceil \log S \rceil$ is necessary in order to allow access to any piece of memory which is used during the computation.
	We usually assume that $w = \Theta(\log S)$ as working with larger words would make the model too powerful.
	\item Constant number of \textbf{registers}; each of them stores $w$ bits, which we call a \emph{machine word} or simply a \emph{word}.
\end{itemize}

The instructions supported by the RAM are:
\begin{enumerate}
	\item Program flow control used for determining what instruction to process next.
	\begin{itemize}
		\item \textbf{Termination}: $\textrm{halt}$ \\
		Stops the program immediately.
		\item \textbf{Jumps}: $label{:}\ instruction, \textrm{goto}\ label, \textrm{goto}\ label\ \textrm{if}\ a$ \\
		Continues with execution of the instruction at the specified label.
		In the latter case, it goes to the label only if $a$ is $true$, otherwise it continues with the next instruction.
		\item \textbf{Functions}: $\textrm{function}\ fn(args\ldots), \textrm{return}\ a$ \\
		Defines a function $fn$ with a constant number of arguments, possibly zero.
		A function has access to the main memory but not to any registers which were used by its caller.
		Each function ends with a return instruction followed by a value.
		After a return, the program continues with an instruction which follows the one which initiated the call.
	\end{itemize}
	\item Work with memory:
	\begin{itemize}
		\item \textbf{Read}: $register \gets Memory[position, length]$ \\
		Reads $length \le w$ consecutive bits from memory starting at $position$.
		Once they are read, it stores them in a named $register$ right aligned and padded with zeros from left.
		\item \textbf{Write}: $Memory[position, length] \gets a$ \\
		Overwrites $length$ bits of memory with the $length$ least significant bits of the value of $a$ in memory at given $position$.
		\item \textbf{Section}: $register \gets a[position, length]$ \\
		Similar to reading from memory but operates on registers.
	\end{itemize}
	\item Operators on registers; the resulting value is always stored to a register: $register \gets operation$:
	\begin{itemize}
		\item \textbf{Arithmetic}: $a + b, a - b, a \cdot b, a \mathbin{/} b, a \% b$ \\
		All overflows are silently discarded; the division is integral.
		\item \textbf{Bitwise}: $a \bitand b, a \bitor b, a \bitxor b, \bitnot a, a \bitlsh b, a \bitrsh b$ \\
		Bit shifts are padded with zeros.
		\item \textbf{Boolean}: $a \booland b, a \boolor b, \boolnot a$ \\
		Integers are implicitly converted to boolean ($0 \Leftrightarrow false$) and booleans to integers ($true \Rightarrow 1; false \Rightarrow 0$).
		\item \textbf{Relational}: $a < b, a \le b, a > b, a \ge b, a = b, a \ne b$ \\
		The result is a boolean value.
		\item \textbf{Function call}: $fn(args\ldots)$ \\
		Calls a function $fn$, provides it required number of arguments.
	\end{itemize}
\end{enumerate}
All single letter arguments can be either names of the registers or constants of size $w$ bits.

\section{Succinct Data Structures}

In a universe $U$ every object can be represented as a sequence of $\lceil \log |U| \rceil$, which we will for convenience call $N$, bits simply by ordering all objects and encoding their number in binary.
Although this encoding is optimal in terms of the space used, it is rarely useful for more advanced operations than iterations over the objects in the universe and reconstruction of the original object.

Many operations cannot be supported unless the object is fully decoded into a more traditional form first; others can be processed on the encoded object however their time complexity is worse than optimal.

An \emph{implicit data structure} for a universe $U$ is a data structure which has space complexity $ \log |U| + O(1) $ bits of space.
Examples of such data structures are heaps implemented as arrays; the internal navigation and realization of operations consists in pure arithmetic computation, swaps, and comparisons.

The part ``bits of space'' is important because in case of implicit data structures, the additional constant number of bits don't even allow to represent a pointer.
Size of each pointer is word size from the definition of RAM, which is at least $\lceil \log |U| \rceil$ in order to support indexing of the whole memory.

Restriction on at most constant additional space is often too strict.
A \emph{succinct data structures} is a data structure whose space complexity is $ \log |U| + o(\log |U|) $ bits of space.
Examples are some restricted classes of graphs, such as planar graphs, and trees.

A \emph{compact data structure} is a data structure which uses $ O(\log |U|) $ bits of space.
Sometimes it is easier to design a compact rather than a succinct data structure.
Edges in general graphs have two ends; in order to support efficient traversal of the edges, it seems logical to store every edge twice.

% TODO do we need the last clause?
From now on, we will describe only succinct data structures, unless we state otherwise.

\section{Storage}

There are two distinct approaches to representing data structures as bit strings depending on how they are stored.
We formulate them in terms of graphs.

The \emph{systematic approach} splits the encoding into two parts: the data sufficient for reconstruction of the data structure, and auxiliary indices which help to realize or speed up queries on the data structure.
The first part is usually made by traversal or decomposition of the data structure and itself can answer to some queries on neighborhood of the active vertex.
Indices then store additional information which is often intended to offer precomputed jumps for queries which cannot be answered locally.
The size of the indices is dominated by the size of the data.

The \emph{non-systematic encoding} cannot be split into such two parts.
The organization of data itself allows all kinds of queries to be supported in desired time.

Both approaches have advantages and disadvantages.
In systematic approach it is easy to add another index and thereby support new operations.
It is also possible to drop indices which are not necessary for particular use-cases making the encoded data structure a little smaller.
Sometimes it can be beneficial to save only the data while computing all of the indices when the data structure is first used.

Non-systematic representations are designed with the set of supported operations on one's mind.
It is not possible to remove support for any operation in order to spare some space.
Adding support of a new operation which was not planned beforehand requires its composition of from the supported operations or adding an index, which results in a hybrid data structure.

\section{Basic Techniques}

There are a few techniques which are used in every data structure.
The idea of most of them is simple, however when combined together they pose a strong tool.

\subsection{Pointers}

Succinct data structure is a string of bits.
In order to give it a meaning, we often needs to think about parts of the string: the data and the indices.

\begin{lemma}
	Let $K = o(\frac{N}{\log N})$ be number of bit strings $S_1, \ldots, S_K$ and $N_1, \dots, N_K$ their lengths which sum up to $N$.
	Then we can combine all bit strings into a single bit string $S$ while maintaining succinctness and constant time access to the individual bit strings.
	Typically $K = O(1)$.
\end{lemma}
\begin{proof}
	The new bit string $S$ starts with a header in form of a table and continues with concatenated bit strings $S_1, \ldots, S_K$ without any delimiters.
	The table contains $K$ offsets of all the original bit strings within $S$ ignoring the space taken by the header.
	Each offset is encoded as a binary number using $\lceil \log N \rceil$ bits, which results in the table having size $K \lceil \log (N) \rceil = o(N)$.
\end{proof}

We stored offsets of the bits where the individual substrings start.
We can look at this offset as a local pointer whose size depends on the range which it shall point into.

Pointers which have different sizes allow us to decompose a big structure into small blocks.
There will be a lot of pointers withing blocks, however they require only a small size, and pointers between blocks, which are large in space but used only seldom.

\subsection{Precomputation}

The goal of most succinct data structures is to perform queries in constant time.
Even if this was possible on high level, in the end the query must be processed on data of size of a machine word.

Let's assume that we have a block of data of $B$ bits.
Let's also have parameters of the query on block of such size; these parameters have size $P$ bits in total.
And an expected result of size $R$ bits.
As long as $B+P+\log R = o(\log N)$, we can afford to precompute result of all possible queries while still having a succinct data structure.

We represent this as a table -- denoted as a triplet $(B, P, R)$ -- with records of size $R$ bits.
There are exactly $2^{B+P} = o(N)$ such records.
Lookup is easy: $B$ and $P$ bits are concatenated and multiplied by $R$, which gives the offset of $R$ bits of result.

% TODO four Russians
This technique is known as \url{https://en.wikipedia.org/wiki/Method_of_Four_Russians}.

\subsubsection{\label{sss:precomputation-example}Example}

Assume a bit string of length $N$ bits.
For each block of $B = \lceil\frac{\log N}{2}\rceil$ bits, we want to run a range query asking for number of ones.
The query has a parameter $end$ of size $\lceil \log {B} \rceil$ bits, and result of the same size.
The size of the table is then $O(\sqrt{N} \log N \log\log N) = o(N)$.

This example is one of building blocks used in design of the \emph{rank index}.

\subsection{\label{ss:index-without-data}Index Without Data}

Let's assume that we want to support an operation on derived data.
Instead of counting ones, as in the previous example, we could want to count transitions from zero to one in bit string $S$.
We can define a new bit string $T$ as:
$$ T = \bitnot S \bitand (S \bitlsh 1)$$
Any $w$ consecutive bits can be calculated in constant time; mind the adjustment for overflows and string ends.

And on top of this bit string we build the rank index.
Although the index is small, $o(N)$, the data is too big $N$, making our data structure compact and not succinct.

We solve it by keeping the small index but dropping the data.
Whenever the algorithm asks for a block the data, it only wants $w$ bits at a time, which we can compute on the fly with a constant slow down.
This technique works in general; we don't need to know how the operation works as long as we are able to provide it on request any chunk of $w$ bits of memory.

\subsection{Space Complexity Analysis}

Sometimes it is easier to think about \emph{density} of an index (or its part) in bits per bit of data.
Then a succinct index is such index which has a constant number of parts with each of them accounting to $o(1)$ bits per $1$ bit of data.